#!/usr/bin/env python3
import os
import re
import logging
from io import BytesIO, StringIO
from typing import List, Dict, Optional, Any, Tuple
import csv
import json
import time

import uuid
import requests
import pandas as pd
import streamlit as st
from docx import Document
try:
    from openai import OpenAI
    import openai
except Exception:
    OpenAI = None  # optional
    openai = None

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class AppConfig:
    """åº”ç”¨ç¨‹åºé…ç½®"""
    # åŸºæœ¬å‚æ•°
    DEFAULT_HEADERS = ["æµ‹è¯•åç§°", "éœ€æ±‚ç¼–å·", "éœ€æ±‚æè¿°", "æµ‹è¯•æè¿°", "å‰ç½®æ¡ä»¶", "æµ‹è¯•æ­¥éª¤", "é¢„æœŸç»“æœ", "éœ€æ±‚è¿½æº¯"]
    DEFAULT_BASE_URL = "http://model.mify.ai.srv"  # å†…éƒ¨æœåŠ¡ä¼˜å…ˆ
    MAX_RETRY_ATTEMPTS = 3
    MIN_PARAGRAPH_LENGTH = 10
    API_KEY = "sk-HXFiS9bEeg95uypM96B6kJfKaxe3ze52FUeQEriGGaGIIefS"  # å›ºå®šç¡¬ç¼–ç 

    # æ¨¡å‹é…ç½®
    MODEL_MAP = {
        "MiMo-7B-RL": "MiMo-7B-RL",
        "Qwen-235B-A22B": "Qwen-235B-A22B", 
        "deepseek-v3.1": "deepseek-v3.1",
        "Qwen2.5-VL-72B-Instruct-AWQ": "Qwen2.5-VL-72B-Instruct-AWQ",
        "mock-model": "mock-model",
    }
    
    ALLOWED_MODELS = list(MODEL_MAP.keys())
    
    MODEL_PRICING_TAG = {
        "MiMo-7B-RL": "(å…è´¹)",
        "Qwen-235B-A22B": "(è®¡è´¹)", 
        "deepseek-v3.1": "(è®¡è´¹)",
        "Qwen2.5-VL-72B-Instruct-AWQ": "(è®¡è´¹)"
    }
    
    # æœåŠ¡è·¯ç”±é…ç½®
    MODEL_PROVIDER_HEADER = {
        "MiMo-7B-RL": "xiaomi",
        "Qwen-235B-A22B": "openai_api_compatible",
        "deepseek-v3.1": "openai_api_compatible",
        "Qwen2.5-VL-72B-Instruct-AWQ": "openai_api_compatible"
    }



def handle_errors(func):
    def wrapper(*args, **kwargs):
        try:
            return func(*args, **kwargs)
        except Exception as e:
            logger.exception(e)
            msg = str(e)
            low = msg.lower()
            # Detect common authentication failure patterns
            if '401' in msg or 'authentication' in low or 'invalid' in low and 'key' in low or 'invalid_request_error' in low:
                st.error("è®¤è¯å¤±è´¥ï¼šAPI Key æ— æ•ˆæˆ–æœªæˆæƒã€‚è¯·åœ¨ä¾§è¾¹æ é‡æ–°è¾“å…¥æ­£ç¡®çš„ API Keyï¼Œæˆ–é€‰æ‹© 'local-model' / 'mock-model' è¿›è¡Œæœ¬åœ°æµ‹è¯•ã€‚")
            else:
                st.error(f"æ“ä½œå¤±è´¥: {msg}")
            return None
    return wrapper


@handle_errors
def read_word(file) -> str:
    doc = Document(file)
    paras = [p.text.strip() for p in doc.paragraphs if p.text and p.text.strip()]
    content = "\n".join(paras)
    if not content.strip():
        raise ValueError("Word æ–‡æ¡£ä¸ºç©º")
    return content


@handle_errors
def read_excel(uploaded_file) -> Dict[str, pd.DataFrame]:
    xl = pd.ExcelFile(uploaded_file)
    sheets = {}
    for sheet in xl.sheet_names:
        df = xl.parse(sheet)
        if df.empty:
            continue
        sheets[sheet] = df
    if not sheets:
        raise ValueError("Excel æ²¡æœ‰æœ‰æ•ˆå·¥ä½œè¡¨")
    return sheets


def build_prompt(requirement: str, headers: List[str], pos_n: int, neg_n: int, edge_n: int, req_id: str = "", background_knowledge: Optional[str] = None) -> str:
    if not requirement.strip():
        raise ValueError("éœ€æ±‚ä¸èƒ½ä¸ºç©º")
    cols_line = ",".join(headers)
    total_cases = pos_n + neg_n + edge_n

    background_section = ""
    if background_knowledge and background_knowledge.strip():
        background_section = f"""
è¯·å‚è€ƒä»¥ä¸‹èƒŒæ™¯çŸ¥è¯†æ¥ç”Ÿæˆç”¨ä¾‹ï¼š
---
{background_knowledge.strip()}
---
"""

    guidance = f"""
{background_section}
ä½ æ˜¯ä¸€åå…·å¤‡ç”µåŠ›ç”µå­ä¸è½¦è½½ç³»ç»Ÿç»éªŒçš„é«˜çº§æµ‹è¯•å·¥ç¨‹å¸ˆï¼Œç†Ÿæ‚‰ OBC/CCU/BMS/EVCCã€CAN/CAN-FDã€å……ç”µæµç¨‹ä¸åŠŸç‡çº¦æŸã€‚
è¯·åŸºäºä¸‹åˆ—éœ€æ±‚ç”Ÿæˆ {total_cases} æ¡é«˜è´¨é‡ã€å¯æ‰§è¡Œçš„æµ‹è¯•ç”¨ä¾‹ï¼ˆCSV æ ¼å¼ï¼Œç¬¬ä¸€è¡Œä¸ºè¡¨å¤´ï¼‰ï¼š
{cols_line}

åˆ†é…ï¼šæ­£å‘ {pos_n} æ¡ï¼Œå¼‚å¸¸ {neg_n} æ¡ï¼Œè¾¹ç•Œ {edge_n} æ¡ã€‚

è§„åˆ™ï¼š
- ä»…è¾“å‡º CSV å†…å®¹ï¼Œä¸è¦é™„åŠ è§£é‡Šæˆ–ä»£ç å—ã€‚
- æµ‹è¯•æ­¥éª¤ç”¨åˆ†å·ï¼ˆï¼›ï¼‰åˆ†éš”å¹¶æ”¾åœ¨åŒä¸€å•å…ƒæ ¼å†…ã€‚
- å‰ç½®æ¡ä»¶ä¸ºç©ºå¡«å†™ "æ— "ã€‚
- è¾“å…¥æ•°æ®è¦å…·ä½“ï¼ˆä¾‹å¦‚ï¼šVIN=1234, CAN_ID=0x18FF50E5, ç”µå‹=400V, ç”µæµ=50Aï¼‰ã€‚
- é¢„æœŸç»“æœåº”åŒ…å«å¯è§‚æµ‹çš„é˜ˆå€¼æˆ–æ—¶é—´æ¡ä»¶ï¼ˆä¾‹å¦‚ï¼šç”µæµç¨³å®šåœ¨ 50A Â±5% æŒç»­ 10sï¼‰ã€‚
- éœ€æ±‚ç¼–å·åˆ—å¡«å†™: {req_id if req_id else "REQ-001"}
- éœ€æ±‚æè¿°åˆ—ç®€è¦æ¦‚æ‹¬éœ€æ±‚å†…å®¹ï¼ˆä¸è¶…è¿‡50å­—ï¼‰
- éœ€æ±‚è¿½æº¯åˆ—å¡«å†™è¯¥æµ‹è¯•ç”¨ä¾‹éªŒè¯çš„å…·ä½“éœ€æ±‚ç‚¹

ç”µåŠ›ç”µå­æ³¨æ„äº‹é¡¹ï¼šæ˜ç¡®é‡‡æ ·æ—¶åºã€SOC/æ¸©åº¦/ç”µåŠ›è¾¹ç•Œã€æ•…éšœæ³¨å…¥ï¼ˆä¸¢å¸§/å»¶è¿Ÿ/çŸ­è·¯ï¼‰ã€EVCCé€šä¿¡åè®®å’Œå®‰å…¨äº’é”ã€‚
"""

    return f"{guidance}\n\néœ€æ±‚ID: {req_id}\néœ€æ±‚æè¿°:\n{requirement.strip()}\n\nè¯·å¼€å§‹ç”Ÿæˆæµ‹è¯•ç”¨ä¾‹ï¼š"


def get_requirement_templates() -> Dict[str, str]:
    return {
        "OBC å……ç”µæµç¨‹": """
REQ-OBC-001: ã€åŠŸèƒ½ã€‘è½¦è½½å……ç”µæœº (OBC)ï¼šæ’æªæ¡æ‰‹->æˆæƒ->å……ç”µ->åœæ­¢
åœºæ™¯åŒ…æ‹¬ï¼šæ¥åœ°æ£€æµ‹ã€äº’é”ã€é™æµã€å……ç”µå®Œæˆæ£€æµ‹ä¸æ•…éšœå¤„ç†
éªŒè¯ç‚¹ï¼šæ¡æ‰‹æ—¶åºã€æˆæƒæµç¨‹ã€å……ç”µå‚æ•°åå•†ã€å¼‚å¸¸æ–­å¼€å¤„ç†
""",
        "CCU ä¸ BMS äº¤äº’": """
REQ-CCU-001: ã€åŠŸèƒ½ã€‘CCU è¯·æ±‚ BMS çŠ¶æ€ï¼ˆSOC/æ¸©åº¦/ç”µå‹/æ•…éšœç ï¼‰ï¼Œå¤„ç†è¶…æ—¶ä¸é‡è¯•
éªŒè¯ç‚¹ï¼šCANé€šä¿¡æ—¶åºã€æ•°æ®å®Œæ•´æ€§ã€è¶…æ—¶é‡è¯•æœºåˆ¶ã€æ•…éšœç è§£æ
""",
        "BMS SOC ä¸å……æ”¾ç”µç­–ç•¥": """
REQ-BMS-001: ã€åŠŸèƒ½ã€‘SOC ä¼°ç®—ã€æ¸©åº¦ç›¸å…³å……æ”¾ç”µé™åˆ¶ã€ä½ç”µé‡ä¿æŠ¤
éªŒè¯ç‚¹ï¼šSOCç²¾åº¦ã€æ¸©åº¦ä¿æŠ¤é˜ˆå€¼ã€åŠŸç‡é™åˆ¶ç®—æ³•ã€ä¿æŠ¤ç­–ç•¥è§¦å‘
""",
        "EVCC é€šä¿¡æ§åˆ¶": """
REQ-EVCC-001: ã€åŠŸèƒ½ã€‘EVCCä¸å……ç”µæ¡©é€šä¿¡ï¼šISO15118åè®®ã€æ•°å­—è¯ä¹¦éªŒè¯ã€å……ç”µå‚æ•°åå•†
éªŒè¯ç‚¹ï¼šåè®®æ¡æ‰‹ã€è¯ä¹¦é“¾éªŒè¯ã€å‚æ•°åå•†ã€é€šä¿¡å®‰å…¨æ€§
""",
        "å……ç”µè¿æ¥ä¸æ–­å¼€æµç¨‹": """
REQ-CHG-001: ã€åŠŸèƒ½ã€‘äººæœºä¸ç¡¬ä»¶äº¤äº’ï¼šæ’æªã€æˆæƒã€å¼€å§‹ã€å®Œæˆã€æ‹”æªä¸å¼ºåˆ¶ä¸­æ–­åœºæ™¯
éªŒè¯ç‚¹ï¼šç‰©ç†è¿æ¥æ£€æµ‹ã€ç”¨æˆ·æˆæƒã€å……ç”µå¯åœã€ç´§æ€¥æ–­å¼€
""",
    }


def get_requirement_examples() -> List[str]:
    return [
        "OBC: æ’æªå 5s å†…æœªæˆæƒåº”å–æ¶ˆè¯·æ±‚",
        "BMS: æ¸©åº¦>60Â°C æ—¶é™åˆ¶å……ç”µç”µæµè‡³ 0.2C",
        "CCU: BMS è¯·æ±‚è¶…æ—¶ 100ms åé‡è¯• 3 æ¬¡å¹¶è®°å½•æ•…éšœ",
    ]


def compute_dynamic_case_counts(
    text: str,
    min_total: int = 3,
    max_total: int = 9,
    pos_w: float = 3.0,
    neg_w: float = 2.0,
    edge_w: float = 2.0
) -> Tuple[int, int, int]:
    """æ ¹æ®éœ€æ±‚æ–‡æœ¬å¤æ‚åº¦åŠ¨æ€è®¡ç®—å„ç±»ç”¨ä¾‹æ•°é‡"""
    # è®¡ç®—åŸºç¡€å¤æ‚åº¦åˆ†æ•° (æ ¹æ®æ–‡æœ¬é•¿åº¦å’Œå¥å­æ•°)
    sentences = len(re.split(r'[ã€‚ï¼ï¼Ÿ.!?]+', text.strip()))
    words = len(text.strip())
    base_score = min(1.0, words / 1000) * 0.6 + min(1.0, sentences / 10) * 0.4
    
    # é£é™©å…³é”®è¯åŠ æƒ (æ¯ä¸ªå…³é”®è¯æé«˜10%çš„å¤æ‚åº¦, æœ€é«˜åˆ°2.0)
    risk_keywords = [
        "å¼‚å¸¸", "æ•…éšœ", "é”™è¯¯", "è¶…æ—¶", "é‡è¯•", "ä¿æŠ¤", "è¾¹ç•Œ",
        "é™åˆ¶", "å®‰å…¨", "è­¦å‘Š", "æŠ¥è­¦", "é”™è¯¯", "è¯Šæ–­", "ä¸¢å¸§"
    ]
    keyword_matches = sum(1 for k in risk_keywords if k in text)
    risk_score = min(2.0, 1.0 + keyword_matches * 0.1)
    
    # æœ€ç»ˆå¤æ‚åº¦åˆ†æ•° (åŸºç¡€åˆ†æ•°å’Œé£é™©åˆ†æ•°çš„åŠ æƒå¹³å‡)
    complexity = base_score * 0.7 + risk_score * 0.3
    
    # æ ¹æ®å¤æ‚åº¦è®¡ç®—ç”¨ä¾‹æ€»æ•° (åœ¨min_totalå’Œmax_totalä¹‹é—´çº¿æ€§æ’å€¼)
    total_cases = round(min_total + (max_total - min_total) * complexity)
    
    # æŒ‰æƒé‡åˆ†é…ç”¨ä¾‹æ•°
    total_weight = pos_w + neg_w + edge_w
    pos_ratio = pos_w / total_weight
    neg_ratio = neg_w / total_weight
    edge_ratio = edge_w / total_weight
    
    pos = round(total_cases * pos_ratio)
    neg = round(total_cases * neg_ratio)
    edge = round(total_cases * edge_ratio)
    
    # ç¡®ä¿æ¯ç±»è‡³å°‘1ä¸ªç”¨ä¾‹
    pos = max(1, pos)
    neg = max(1, neg)
    edge = max(1, edge)
    
    return pos, neg, edge

def _generate_mock_csv(requirement: str, headers: List[str], pos_n: int, neg_n: int, edge_n: int, req_id: str = "") -> str:
    """Generate a deterministic mock CSV string for fast local testing."""
    rows = []
    idx = 1
    def make_row(req_num, req_desc, title, desc, pre, steps, expect, trace):
        # escape double quotes by doubling them for CSV
        cells = [req_num, req_desc, title, desc, pre or "æ— ", steps, expect, trace]
        quoted = [f'"{c.replace("\"", "\"\"")}"' for c in cells]
        return ",".join(quoted)

    # Extract requirement ID from text if present
    req_match = re.search(r'REQ-[A-Z]+-\d+', requirement)
    final_req_id = req_match.group(0) if req_match else (req_id if req_id else "REQ-001")
    req_desc = requirement[:50] + "..." if len(requirement) > 50 else requirement

    for i in range(pos_n):
        title = f"{requirement[:30]} - æ­£å‘ {i+1}"
        desc = f"éªŒè¯æ­£å¸¸æµç¨‹ {i+1}"
        steps = "æ­¥éª¤1ï¼šåˆå§‹åŒ–ï¼›æ­¥éª¤2ï¼šæ‰§è¡Œï¼›æ­¥éª¤3ï¼šéªŒè¯"
        expect = "åŠŸèƒ½æŒ‰é¢„æœŸï¼›æ— é”™è¯¯"
        trace = f"éªŒè¯ {final_req_id} æ­£å‘åŠŸèƒ½ {i+1}"
        rows.append(make_row(final_req_id, req_desc, title, desc, "æ— ", steps, expect, trace))
        idx += 1
    for i in range(neg_n):
        title = f"{requirement[:30]} - å¼‚å¸¸ {i+1}"
        desc = f"éªŒè¯å¼‚å¸¸å¤„ç† {i+1}"
        steps = "æ­¥éª¤1ï¼šæ³¨å…¥å¼‚å¸¸ï¼›æ­¥éª¤2ï¼šè§‚å¯Ÿï¼›æ­¥éª¤3ï¼šæ¢å¤"
        expect = "äº§ç”Ÿé”™è¯¯ç ï¼›è¿›å…¥å®‰å…¨æ¨¡å¼"
        trace = f"éªŒè¯ {final_req_id} å¼‚å¸¸å¤„ç† {i+1}"
        rows.append(make_row(final_req_id, req_desc, title, desc, "æ³¨å…¥å¼‚å¸¸", steps, expect, trace))
        idx += 1
    for i in range(edge_n):
        title = f"{requirement[:30]} - è¾¹ç•Œ {i+1}"
        desc = f"éªŒè¯è¾¹ç•Œæ¡ä»¶ {i+1}"
        steps = "æ­¥éª¤1ï¼šè®¾ç½®è¾¹ç•Œå€¼ï¼›æ­¥éª¤2ï¼šæ‰§è¡Œï¼›æ­¥éª¤3ï¼šéªŒè¯"
        expect = "ç³»ç»Ÿåœ¨ä¸´ç•Œå€¼ä¸‹ç¨³å®šæˆ–æŒ‰è§„æ ¼å¤„ç†"
        trace = f"éªŒè¯ {final_req_id} è¾¹ç•Œæ¡ä»¶ {i+1}"
        rows.append(make_row(final_req_id, req_desc, title, desc, "æ— ", steps, expect, trace))
        idx += 1

    header = ",".join([f'"{h}"' for h in headers])
    return header + "\n" + "\n".join(rows)



@handle_errors
def call_model(model: str, prompt: str, api_key: str, base_url: str, temperature: float = 0.2, local_model_url: Optional[str] = None, http_proxy: Optional[str] = None, https_proxy: Optional[str] = None) -> str:
    """
    Calls the specified model via HTTP POST request.
    This function handles remote (OpenAI-like & Gemini) and local models.
    """
    # Validate inputs
    if not model:
        raise ValueError("å¿…é¡»æŒ‡å®šæ¨¡å‹åç§°")
        
    if model not in AppConfig.MODEL_MAP and model not in ["local-model", "mock-model", "gemini"]:
        raise ValueError(f"ä¸æ”¯æŒçš„æ¨¡å‹: {model}")
        
    proxies = {}
    if http_proxy and http_proxy.strip():
        proxies["http"] = http_proxy.strip()
    if https_proxy and https_proxy.strip():
        proxies["https"] = https_proxy.strip()
    elif http_proxy and http_proxy.strip(): # Fallback for https
        proxies["https"] = http_proxy.strip()

    # --- Handle Gemini API ---
    if model == "gemini":
        if not api_key: raise ValueError("Gemini æ¨¡å‹éœ€è¦ API Key")
        if not base_url: raise ValueError("Gemini æ¨¡å‹éœ€è¦ API Base URL")

        actual_model = MODEL_MAP.get(model, model)
        url = f"{base_url.rstrip('/')}/v1beta/models/{actual_model}:generateContent?key={api_key}"
        headers = {"Content-Type": "application/json"}
        payload = {
            "contents": [{"parts": [{"text": prompt}]}],
            "generationConfig": {"temperature": temperature}
        }

        for attempt in range(MAX_RETRY_ATTEMPTS):
            try:
                r = requests.post(url, headers=headers, json=payload, timeout=60, proxies=proxies if proxies else None)
                r.raise_for_status()
                j = r.json()
                return j['candidates'][0]['content']['parts'][0]['text']
            except requests.exceptions.HTTPError as e:
                # Handle 429 Rate Limit Exceeded with exponential backoff
                if e.response.status_code == 429:
                    if attempt < MAX_RETRY_ATTEMPTS - 1:
                        wait_time = 2 ** (attempt + 1)  # Exponential backoff: 2, 4, 8 seconds
                        logger.warning(f"Gemini API é€Ÿç‡é™åˆ¶ã€‚å°†åœ¨ {wait_time} ç§’åé‡è¯•... (å°è¯• {attempt + 1}/{MAX_RETRY_ATTEMPTS})")
                        st.toast(f"Gemini API é€Ÿç‡é™åˆ¶ã€‚å°†åœ¨ {wait_time} ç§’åé‡è¯•...")
                        time.sleep(wait_time)
                        continue  # Continue to the next attempt
                    else:
                        st.error("Gemini API é€Ÿç‡é™åˆ¶è¿‡äºé¢‘ç¹ï¼Œè¯·ç¨åå†è¯•æˆ–æ£€æŸ¥æ‚¨çš„è´¦æˆ·é…é¢ã€‚")
                        raise e # Raise on the last attempt
                elif e.response.status_code == 404:
                    error_message = (
                        "Gemini API è¿”å› 404 Not Found é”™è¯¯ã€‚\n"
                        "è¿™é€šå¸¸æ„å‘³ç€ä»¥ä¸‹é—®é¢˜ä¹‹ä¸€ï¼š\n"
                        "1. **API Base URL ä¸æ­£ç¡®**: è¯·ç¡®ä¿ä¾§è¾¹æ ä¸­çš„ URL æ˜¯ `https://generativelanguage.googleapis.com`ã€‚\n"
                        "2. **API Key æ— æ•ˆæˆ–æœªæˆæƒ**: è¯·æ£€æŸ¥æ‚¨çš„ API Key æ˜¯å¦æ­£ç¡®ï¼Œå¹¶ç¡®ä¿å®ƒæ‰€å±çš„ Google Cloud é¡¹ç›®å·²ç»å¯ç”¨äº† 'Generative Language API' æˆ– 'Gemini API'ã€‚\n"
                        "3. **æ¨¡å‹åç§°ä¸æ­£ç¡®**: ç¡®è®¤æ¨¡å‹åç§° `gemini-1.5-pro-latest` æ˜¯å¦é€‚ç”¨äºæ‚¨çš„ API Keyã€‚"
                    )
                    raise ValueError(error_message) from e
                # Re-raise other HTTP errors
                raise e
            except (requests.exceptions.RequestException, KeyError, IndexError) as e:
                logger.warning(f"Gemini æ¨¡å‹è°ƒç”¨ (URL: {url}) ç¬¬ {attempt+1} æ¬¡å¤±è´¥: {e}")
                if attempt == MAX_RETRY_ATTEMPTS - 1:
                    raise e
        raise Exception("Gemini æ¨¡å‹è°ƒç”¨å¤±è´¥ï¼Œå·²è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°")

    # --- Handle local & OpenAI-compatible APIs ---
    if model == "local-model":
        if not local_model_url: raise ValueError("ä½¿ç”¨ local-model æ—¶éœ€æä¾› local_model_url")
        url = local_model_url
        headers = {"Content-Type": "application/json"}
        payload = {"prompt": prompt, "temperature": temperature}
    else: # OpenAI-compatible remote models
        if not api_key: raise ValueError("è¿œç«¯æ¨¡å‹éœ€è¦ API Key")
        if not base_url: raise ValueError("è¿œç«¯æ¨¡å‹éœ€è¦ API Base URL")
        url = f"{base_url.rstrip('/')}/v1/chat/completions"
        headers = {
            "Content-Type": "application/json",
            "Authorization": f"Bearer {api_key}",
            "Connection": "keep-alive"
        }

        # ä»AppConfigè·å–è·¯ç”±å¤´
        provider = AppConfig.MODEL_PROVIDER_HEADER.get(model)
        if provider:
            headers["X-Provider"] = provider

        actual_model = AppConfig.MODEL_MAP.get(model, model)
        
        # Add debug logging
        logger.info(f"Calling API endpoint: {url}")
        logger.info(f"Model: {model} (actual: {actual_model})")
        logger.info(f"Headers: {headers}")
        if st.session_state.get("debug_mode"):
            st.write(f"Debug - API Endpoint: {url}")
            st.write(f"Debug - Model: {model} (actual: {actual_model})")
            st.write(f"Debug - Headers: {headers}")
            st.write(f"Debug - Provider: {provider}")
            payload_debug = {
                "model": actual_model,
                "messages": [
                    {"role": "system", "content": "ä½ æ˜¯æµ‹è¯•ç”¨ä¾‹ç”ŸæˆåŠ©æ‰‹ï¼Œä¸¥æ ¼è¾“å‡º CSV"},
                    {"role": "user", "content": prompt[:100] + "..." if len(prompt) > 100 else prompt}
                ],
                "temperature": temperature,
            }
            st.write(f"Debug - Payload: {json.dumps(payload_debug, ensure_ascii=False, indent=2)}")
        payload = {
            "model": actual_model,
            "messages": [{"role": "system", "content": "ä½ æ˜¯æµ‹è¯•ç”¨ä¾‹ç”ŸæˆåŠ©æ‰‹ï¼Œä¸¥æ ¼è¾“å‡º CSV"}, {"role": "user", "content": prompt}],
            "temperature": temperature,
            "max_tokens": 4000,
        }
        # ä»MODEL_PROVIDER_HEADERè·å–è·¯ç”±å¤´
        provider = AppConfig.MODEL_PROVIDER_HEADER.get(model)
        if provider:
            headers["X-Provider"] = provider

    # Make the request with retries for non-Gemini models
    for attempt in range(AppConfig.MAX_RETRY_ATTEMPTS):
        try:
            r = requests.post(url, headers=headers, json=payload, timeout=60, proxies=proxies if proxies else None)
            r.raise_for_status()
            j = r.json()
            if model == "local-model":
                 return j.get("text") or j.get("output") or j.get("result") or r.text
            else:
                return j['choices'][0]['message']['content']
        except requests.exceptions.HTTPError as e:
            # Handle 429 Rate Limit Exceeded with exponential backoff
            if e.response.status_code == 429:
                if attempt < AppConfig.MAX_RETRY_ATTEMPTS - 1:
                    wait_time = 2 ** (attempt + 1)  # Exponential backoff: 2, 4, 8 seconds
                    logger.warning(f"API é€Ÿç‡é™åˆ¶ã€‚å°†åœ¨ {wait_time} ç§’åé‡è¯•... (å°è¯• {attempt + 1}/{AppConfig.MAX_RETRY_ATTEMPTS})")
                    st.toast(f"API é€Ÿç‡é™åˆ¶ã€‚å°†åœ¨ {wait_time} ç§’åé‡è¯•...")
                    time.sleep(wait_time)
                    continue  # Continue to the next attempt
                else:
                    st.error("API é€Ÿç‡é™åˆ¶è¿‡äºé¢‘ç¹ï¼Œè¯·ç¨åå†è¯•æˆ–æ£€æŸ¥æ‚¨çš„è´¦æˆ·é…é¢ã€‚")
                    raise e # Raise on the last attempt
            # For other HTTP errors, re-raise immediately
            raise e
        except (requests.exceptions.RequestException, KeyError, IndexError) as e:
            logger.warning(f"æ¨¡å‹è°ƒç”¨ (URL: {url}) ç¬¬ {attempt+1} æ¬¡å¤±è´¥: {e}")
            if attempt == AppConfig.MAX_RETRY_ATTEMPTS - 1:
                raise e
    raise Exception("æ¨¡å‹è°ƒç”¨å¤±è´¥ï¼Œå·²è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°")


@handle_errors
def parse_csv_to_df(csv_text: str, expected_headers: List[str]) -> pd.DataFrame:
    if not csv_text or not csv_text.strip():
        raise ValueError("CSV å†…å®¹ä¸ºç©º")
    cleaned = csv_text.strip()
    cleaned = re.sub(r"^```.*?\n", "", cleaned, flags=re.MULTILINE)
    cleaned = re.sub(r"\n```$", "", cleaned)
    cleaned = cleaned.replace("\ufeff", "")
    lines = [l for l in cleaned.splitlines() if l.strip()]
    if not lines:
        raise ValueError("CSV å†…å®¹ä¸ºç©ºï¼ˆæ¸…ç†åï¼‰")

    text = "\n".join(lines)

    # Try to detect delimiter (comma, semicolon, tab, pipe)
    try:
        sniffer = csv.Sniffer()
        dialect = sniffer.sniff(text[:4096], delimiters=",;\t|")
        delimiter = dialect.delimiter
    except Exception:
        delimiter = ','

    # Parse using csv.reader to respect quotes robustly
    reader = csv.reader(StringIO(text), delimiter=delimiter, quotechar='"')
    rows = [r for r in reader if any(cell.strip() for cell in r)]
    if not rows:
        raise ValueError("CSV å†…å®¹æ— æ³•è§£æä¸ºè¡Œ")

    # helper to normalize rows to a target column count
    def _normalize_rows(rows_list, n_cols, delim):
        normalized = []
        for r in rows_list:
            # strip each cell
            r = [c.strip().strip('"') for c in r]
            if len(r) <= n_cols:
                normalized.append(r + [""] * (n_cols - len(r)))
            else:
                # merge extra columns into the last column to avoid misalignment
                merged_last = delim.join(r[n_cols - 1:])
                normalized.append(r[:n_cols - 1] + [merged_last])
        return normalized

    # Heuristics to determine header row
    header_row = 0
    header = [c.strip().strip('"') for c in rows[0]]
    # if header looks like expected (contains some expected header names), use it
    matches = sum(1 for h in header if any(exp in h or h in exp for exp in expected_headers))
    if matches >= max(1, len(expected_headers) // 2):
        # ensure all data rows match header length
        data_rows = rows[1:]
        if not all(len(r) == len(header) for r in data_rows):
            data_rows = _normalize_rows(data_rows, len(header), delimiter)
        df = pd.DataFrame(data_rows, columns=header)
    else:
        # try to find a header in the first 3 rows
        found = False
        for i in range(0, min(3, len(rows))):
            r = [c.strip().strip('"') for c in rows[i]]
            matches = sum(1 for h in r if any(exp in h or h in exp for exp in expected_headers))
            if matches >= max(1, len(expected_headers) // 2) and len(r) >= 2:
                header_row = i
                data_rows = rows[i+1:]
                if not all(len(rr) == len(r) for rr in data_rows):
                    data_rows = _normalize_rows(data_rows, len(r), delimiter)
                df = pd.DataFrame(data_rows, columns=r)
                found = True
                break
        if not found:
            # If all rows have the same column count as expected, map directly
            if all(len(r) == len(expected_headers) for r in rows):
                df = pd.DataFrame(rows, columns=expected_headers)
            else:
                # Normalize rows to expected column count by merging extra columns into last column
                normalized = _normalize_rows(rows, len(expected_headers), delimiter)
                df = pd.DataFrame(normalized, columns=expected_headers)

    df = df.fillna("").astype(str)
    return df


def make_excel_download(df: pd.DataFrame, filename: str = "æµ‹è¯•ç”¨ä¾‹.xlsx") -> None:
    # tolerate None
    if df is None or (hasattr(df, "empty") and df.empty):
        st.warning("æ²¡æœ‰æ•°æ®å¯å¯¼å‡º")
        return
    buf = BytesIO()
    with pd.ExcelWriter(buf, engine='openpyxl') as w:
        df.to_excel(w, index=False, sheet_name='æµ‹è¯•ç”¨ä¾‹')
    buf.seek(0)
    st.download_button("ğŸ’¾ ä¸‹è½½ Excel", data=buf, file_name=filename, mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet", key=f"dxl_{uuid.uuid4().hex}")


def make_csv_download(df: pd.DataFrame, filename: str = "æµ‹è¯•ç”¨ä¾‹.csv") -> None:
    # tolerate None
    if df is None or (hasattr(df, "empty") and df.empty):
        st.warning("æ²¡æœ‰æ•°æ®å¯å¯¼å‡º")
        return
    csv = df.to_csv(index=False).encode("utf-8-sig")
    st.download_button("ğŸ’¾ ä¸‹è½½ CSV", data=csv, file_name=filename, mime="text/csv", key=f"dcsv_{uuid.uuid4().hex}")


def process_batch_requirements(api_key: str, base_url: str, requirements: List[str], headers: List[str], model: str, pos_n: int, neg_n: int, edge_n: int, temperature: float, local_model_url: Optional[str], http_proxy: Optional[str], https_proxy: Optional[str], background_knowledge: Optional[str] = None) -> pd.DataFrame:
    all_cases = []
    pb = st.progress(0)
    status = st.empty()
    total = len(requirements)
    for i, req in enumerate(requirements):
        pb.progress((i+1)/total)
        status.text(f"å¤„ç†ä¸­ {i+1}/{total}")
        req_id = f"REQ-{i+1:03d}"
        prompt = build_prompt(req, headers, pos_n, neg_n, edge_n, req_id, background_knowledge)
        text = call_model(model, prompt, api_key, base_url, temperature, local_model_url, http_proxy, https_proxy)
        if text:
            df = parse_csv_to_df(text, headers)
            if df is not None and not df.empty:
                # å¦‚æœç”Ÿæˆçš„æ•°æ®ä¸­æ²¡æœ‰éœ€æ±‚ç¼–å·åˆ—ï¼Œåˆ™æ·»åŠ 
                if "éœ€æ±‚ç¼–å·" not in df.columns:
                    df.insert(0, "éœ€æ±‚ç¼–å·", req_id)
                if "éœ€æ±‚æè¿°" not in df.columns:
                    df.insert(1, "éœ€æ±‚æè¿°", req[:100])
                all_cases.append(df)

        # Add a delay to avoid hitting rate limits, especially for batch jobs.
        # This helps respect policies like "requests per minute".
        if i < total - 1: # No need to wait after the last item
            time.sleep(2) # Wait for 2 seconds before the next request

    pb.empty(); status.empty()
    if all_cases:
        return pd.concat(all_cases, ignore_index=True)
    raise ValueError("æœªç”Ÿæˆä»»ä½•ç”¨ä¾‹")


def setup_sidebar() -> tuple:
    with st.sidebar:
        st.header("è¿æ¥è®¾ç½®")
        api_key = st.text_input("API Keyï¼ˆå¯é€‰ï¼‰", type="password", key="api_key_input")
        model = st.selectbox("æ¨¡å‹", ["deepseek-chat", "chatgpt", "gemini", "local-model", "mock-model"], key="model_select")

        # auto-select sensible base_url based on model
        if model == "chatgpt":
            suggested_base = "https://api.openai.com"
            st.info("å·²ä¸º chatgpt å»ºè®®å°† API Base URL è®¾ç½®ä¸º https://api.openai.comï¼ˆå¯ä¿®æ”¹ï¼‰")
        elif model == "deepseek-chat":
            suggested_base = AppConfig.DEFAULT_BASE_URL
            st.info(f"å·²ä¸º deepseek å»ºè®®å°† API Base URL è®¾ç½®ä¸º {AppConfig.DEFAULT_BASE_URL}ï¼ˆå¯ä¿®æ”¹ï¼‰")
        elif model == "gemini":
            suggested_base = "https://generativelanguage.googleapis.com"
            st.info(f"å·²ä¸º gemini å»ºè®®å°† API Base URL è®¾ç½®ä¸º {suggested_base}ï¼ˆå¯ä¿®æ”¹ï¼‰")
        else:
            suggested_base = DEFAULT_BASE_URL

        base_url = st.text_input("API Base URL", value=suggested_base, key="base_url_input")
        local_model_url = st.text_input("æœ¬åœ°æ¨¡å‹ URL (http)", placeholder="http://127.0.0.1:8000/v1/generate", key="local_model_url")

        st.divider()
        st.subheader("ä»£ç†è®¾ç½® (å¯é€‰)")
        proxy_mode = st.radio("ç½‘ç»œè¿æ¥æ–¹å¼", ["ä½¿ç”¨ç³»ç»Ÿä»£ç†", "è‡ªå®šä¹‰ä»£ç†", "ç›´æ¥è¿æ¥ï¼ˆç»•è¿‡ä»£ç†ï¼‰"], key="proxy_mode")

        http_proxy = None
        https_proxy = None

        if proxy_mode == "ä½¿ç”¨ç³»ç»Ÿä»£ç†":
            http_proxy = "http://127.0.0.1:7897"
            https_proxy = "http://127.0.0.1:7897"
            st.info("å°†ä½¿ç”¨æ£€æµ‹åˆ°çš„ç³»ç»Ÿä»£ç†: 127.0.0.1:7897")
        elif proxy_mode == "è‡ªå®šä¹‰ä»£ç†":
            http_proxy = st.text_input("HTTP Proxy", placeholder="http://user:pass@host:port", key="http_proxy_input")
            https_proxy = st.text_input("HTTPS Proxy", placeholder="http://user:pass@host:port", key="https_proxy_input")
        else:  # ç›´æ¥è¿æ¥
            st.info("å°†ç»•è¿‡ä»£ç†ç›´æ¥è¿æ¥åˆ° API æœåŠ¡å™¨")

        temperature = st.slider("Temperature", 0.0, 1.0, 0.2, 0.05, key="temperature_slider")

        st.divider()
        st.header("èƒŒæ™¯çŸ¥è¯† (å¯é€‰)")
        background_doc = st.file_uploader("ä¸Šä¼ éœ€æ±‚ã€è§„æ ¼æˆ–èƒŒæ™¯çŸ¥è¯†æ–‡æ¡£", type=["docx", "txt", "md"], key="background_doc_uploader")

        # Use a key to avoid re-reading the file on every rerun
        if background_doc:
            if st.session_state.get('last_background_doc_name') != background_doc.name:
                content = read_background_doc(background_doc)
                if content:
                    st.session_state['background_knowledge'] = content
                    st.session_state['last_background_doc_name'] = background_doc.name
                    st.success(f"å·²åŠ è½½èƒŒæ™¯æ–‡æ¡£: {background_doc.name}")
                else:
                    st.session_state['background_knowledge'] = None
                    st.session_state['last_background_doc_name'] = None
        else:
            # Clear if the file is removed by the user
            if 'background_knowledge' in st.session_state:
                st.session_state['background_knowledge'] = None
            if 'last_background_doc_name' in st.session_state:
                st.session_state['last_background_doc_name'] = None

        if st.session_state.get('background_knowledge'):
            with st.expander("æŸ¥çœ‹å·²åŠ è½½çš„èƒŒæ™¯çŸ¥è¯† (å‰500å­—ç¬¦)"):
                st.text(st.session_state['background_knowledge'][:500] + "...")

        st.divider()

        # initialize session_state for validation tracking
        if 'api_valid' not in st.session_state:
            st.session_state['api_valid'] = False
            st.session_state['api_error'] = ''
            st.session_state['api_key_cached'] = ''

        # reset cached validation when the API key text changes
        if api_key and api_key != st.session_state.get('api_key_cached', ''):
            st.session_state['api_valid'] = False
            st.session_state['api_error'] = ''
            st.session_state['api_key_cached'] = api_key

        # validate API Key button
        if st.button("éªŒè¯ API Key", key="validate_api_key"):
            if model in ("local-model", "mock-model"):
                st.info("æ‰€é€‰ä¸ºæœ¬åœ°æ¨¡å‹ï¼Œæ— éœ€éªŒè¯è¿œç«¯ API Keyã€‚")
            else:
                if not api_key:
                    st.error("è¯·å…ˆåœ¨ä¸Šæ–¹è¾“å…¥ API Key å†ç‚¹å‡»éªŒè¯")
                else:
                    try:
                        proxies = {}
                        if http_proxy: proxies['http'] = http_proxy
                        if https_proxy: proxies['https'] = https_proxy

                        # Gemini has a different API structure for validation
                        if model == "gemini":
                            actual_model = MODEL_MAP.get(model, model)
                            ping_url = f"{base_url.rstrip('/')}/v1beta/models/{actual_model}?key={api_key}"
                            ping_headers = {"Content-Type": "application/json"}
                            resp = requests.get(ping_url, headers=ping_headers, proxies=proxies if proxies else None, timeout=20)
                        else: # OpenAI-compatible
                            ping_url = f"{base_url.rstrip('/')}/v1/chat/completions"
                            ping_headers = {
                                "Content-Type": "application/json",
                                "Authorization": f"Bearer {api_key}"
                            }
                            actual_model = AppConfig.MODEL_MAP.get(model, model)
                            ping_payload = {
                                "model": actual_model,
                                "messages": [{"role": "user", "content": "ping"}],
                                "max_tokens": 1,
                            }
                            resp = requests.post(
                                ping_url,
                                headers=ping_headers,
                                json=ping_payload,
                                proxies=proxies if proxies else None,
                                timeout=20
                            )

                        resp.raise_for_status() # Will raise an exception for 4xx/5xx status

                        st.success("éªŒè¯é€šè¿‡ï¼šAPI Key å’Œç½‘ç»œè¿æ¥å¯ç”¨")
                        st.session_state['api_valid'] = True
                        st.session_state['api_error'] = ''
                        st.session_state['api_key_cached'] = api_key
                    except requests.exceptions.RequestException as e:
                        logger.warning(f"API Key éªŒè¯å¤±è´¥: {e}")
                        error_details = f"è¯·æ±‚é”™è¯¯: {e}"
                        if e.response is not None:
                            error_details += f"\nçŠ¶æ€ç : {e.response.status_code}\nå“åº”: {e.response.text}"

                        st.error(f"è®¤è¯æˆ–è¿æ¥å¤±è´¥ï¼š{error_details}")
                        st.session_state['api_valid'] = False
                        st.session_state['api_error'] = str(e)
                    except Exception as e:
                        logger.warning(f"éªŒè¯è¿‡ç¨‹ä¸­å‡ºç°æ„å¤–é”™è¯¯: {e}")
                        st.error(f"éªŒè¯å¤±è´¥: {e}")
                        st.session_state['api_valid'] = False
                        st.session_state['api_error'] = str(e)

        # clear cached validation and rerun
        if st.button("æ¸…é™¤ç¼“å­˜å¹¶é‡ç½®", key="clear_cache"):
            st.session_state['api_valid'] = False
            st.session_state['api_error'] = ''
            st.session_state['api_key_cached'] = ''
            st.experimental_rerun()

        st.divider()
        st.header("ç”¨ä¾‹é…ç½®")
        headers_text = st.text_input("åˆ—åï¼ˆé€—å·åˆ†éš”ï¼‰", value=",".join(DEFAULT_HEADERS), key="headers_input")
        headers = [h.strip() for h in headers_text.split(",") if h.strip()]
        pos_n = st.number_input("æ­£å‘", min_value=1, max_value=20, value=2, key="pos_n_input")
        neg_n = st.number_input("å¼‚å¸¸", min_value=1, max_value=20, value=2, key="neg_n_input")
        edge_n = st.number_input("è¾¹ç•Œ", min_value=1, max_value=20, value=2, key="edge_n_input")
    return api_key, base_url, model, temperature, headers, pos_n, neg_n, edge_n, local_model_url, http_proxy, https_proxy


def main():
    st.set_page_config(page_title="AI æµ‹è¯•ç”¨ä¾‹ç”Ÿæˆå™¨ - ç”µåŠ›ç”µå­", layout="wide")
    st.title("AI æµ‹è¯•ç”¨ä¾‹ç”Ÿæˆå™¨ï¼ˆç”µåŠ›ç”µå­æ–¹å‘ï¼‰")
    api_key, base_url, model, temperature, headers, pos_n, neg_n, edge_n, local_model_url, http_proxy, https_proxy = setup_sidebar()
    tab1, tab2, tab3 = st.tabs(["å•æ¡éœ€æ±‚", "æ‰¹é‡å¤„ç†", "å¸®åŠ©"])

    with tab1:
        st.subheader("å•æ¡éœ€æ±‚ç”Ÿæˆ")
        templates = get_requirement_templates()
        opts = ["è‡ªå®šä¹‰"] + list(templates.keys())
        sel = st.selectbox("æ¨¡æ¿", opts, key="template_select")
        default = templates.get(sel, "") if sel != "è‡ªå®šä¹‰" else ""
        req_text = st.text_area("éœ€æ±‚æè¿°", value=default, height=220, key="requirement_text_area")

        # æ·»åŠ éœ€æ±‚ç¼–å·è¾“å…¥
        req_id = st.text_input("éœ€æ±‚ç¼–å·ï¼ˆå¯é€‰ï¼‰", placeholder="ä¾‹å¦‚: REQ-OBC-001", key="req_id_input")

        if st.button("ç”Ÿæˆ", key="gen_single"):
            if model != "local-model" and model != "mock-model" and not api_key:
                st.error("è¯·è¾“å…¥ API Key æˆ–é€‰æ‹© local-model æˆ– mock-model")
            else:
                prompt = build_prompt(req_text, headers, pos_n, neg_n, edge_n, req_id)
                placeholder = st.empty()
                progress = st.progress(0)
                try:
                    placeholder.info("å¼€å§‹ç”Ÿæˆï¼Œç”¨æ—¶å–å†³äºæ‰€é€‰æ¨¡å‹...")
                    progress.progress(10)
                    if model == "mock-model":
                        text = _generate_mock_csv(req_text, headers, pos_n, neg_n, edge_n, req_id)
                        progress.progress(80)
                    else:
                        text = call_model(model, prompt, api_key, base_url, temperature, local_model_url, http_proxy, https_proxy)
                        progress.progress(80)
                    if text:
                        df = parse_csv_to_df(text, headers)
                        progress.progress(95)
                        if df is None or (hasattr(df, "empty") and df.empty):
                            placeholder.error("æœªèƒ½è§£æä¸ºæœ‰æ•ˆçš„æµ‹è¯•ç”¨ä¾‹è¡¨æ ¼")
                        else:
                            st.dataframe(df, use_container_width=True)
                            make_excel_download(df)
                            make_csv_download(df)
                            progress.progress(100)
                            placeholder.success("ç”Ÿæˆå®Œæˆ")
                finally:
                    progress.empty()
                    placeholder.empty()

    with tab2:
        st.subheader("æ‰¹é‡å¯¼å…¥ï¼ˆExcel/Wordï¼‰")
        uploaded = st.file_uploader("ä¸Šä¼ æ–‡ä»¶", type=["xlsx", "docx"], key="file_uploader")
        if uploaded:
            if uploaded.name.lower().endswith('.xlsx'):
                sheets = read_excel(uploaded)
                sheet = st.selectbox("é€‰æ‹©è¡¨", list(sheets.keys()), key="sheet_select")
                df_sheet = sheets[sheet]
                st.dataframe(df_sheet.head(10))
                col = st.selectbox("éœ€æ±‚åˆ—", list(df_sheet.columns), key="column_select")
                rows = df_sheet[col].dropna().astype(str).str.strip()
                valid = [r for r in rows if len(r) > AppConfig.MIN_PARAGRAPH_LENGTH]
                st.info(f"æ‰¾åˆ° {len(valid)} æ¡æœ‰æ•ˆéœ€æ±‚")
                if st.button("æ‰¹é‡ç”Ÿæˆ", key="batch_gen") and valid:
                    if model == "mock-model":
                        # generate all locally without remote calls
                        all_dfs = []
                        for i, req in enumerate(valid):
                            req_id = f"REQ-{i+1:03d}"
                            txt = _generate_mock_csv(req, headers, pos_n, neg_n, edge_n, req_id)
                            df = parse_csv_to_df(txt, headers)
                            # mock modelå·²ç»åŒ…å«äº†éœ€æ±‚ç¼–å·å’Œæè¿°åˆ—ï¼Œä¸éœ€è¦å†æ·»åŠ 
                            all_dfs.append(df)
                        df_all = pd.concat(all_dfs, ignore_index=True) if all_dfs else pd.DataFrame()
                    else:
                        df_all = process_batch_requirements(api_key, base_url, valid, headers, model, pos_n, neg_n, edge_n, temperature, local_model_url, http_proxy, https_proxy)
                    st.dataframe(df_all)
                    make_excel_download(df_all, "æµ‹è¯•ç”¨ä¾‹_æ‰¹é‡.xlsx")
                    make_csv_download(df_all, "æµ‹è¯•ç”¨ä¾‹_æ‰¹é‡.csv")
            else:
                content = read_word(uploaded)
                parts = re.split(r"\n\s*\n+", content.strip())
                parts = [p for p in parts if len(p.strip()) > AppConfig.MIN_PARAGRAPH_LENGTH]
                st.info(f"è¯†åˆ«åˆ° {len(parts)} æ®µéœ€æ±‚")
                if st.button("æ‰¹é‡ç”Ÿæˆ(æ–‡æ¡£)", key="batch_doc") and parts:
                    if model == "mock-model":
                        all_dfs = []
                        for i, req in enumerate(parts):
                            req_id = f"REQ-DOC-{i+1:03d}"
                            txt = _generate_mock_csv(req, headers, pos_n, neg_n, edge_n, req_id)
                            df = parse_csv_to_df(txt, headers)
                            # mock modelå·²ç»åŒ…å«äº†éœ€æ±‚ç¼–å·å’Œæè¿°åˆ—ï¼Œä¸éœ€è¦å†æ·»åŠ 
                            all_dfs.append(df)
                        df_all = pd.concat(all_dfs, ignore_index=True) if all_dfs else pd.DataFrame()
                    else:
                        df_all = process_batch_requirements(api_key, base_url, parts, headers, model, pos_n, neg_n, edge_n, temperature, local_model_url, http_proxy, https_proxy)
                    st.dataframe(df_all)

    with tab3:
        st.subheader("ç¤ºä¾‹ä¸æœ€ä½³å®è·µ")
        st.write("å¸¸è§ç¤ºä¾‹ï¼š")
        for ex in get_requirement_examples():
            st.write(f"- {ex}")


def make_client(api_key: str, base_url: str, http_proxy: Optional[str] = None, https_proxy: Optional[str] = None) -> Any:
    if OpenAI is None:
        raise ImportError("OpenAI package not installed. Please install it with: pip install openai")
    
    proxies = {}
    if http_proxy:
        proxies["http"] = http_proxy
    if https_proxy:
        proxies["https"] = https_proxy
    elif http_proxy:  # Fallback for https if only http is provided
        proxies["https"] = http_proxy
        
    return OpenAI(
        api_key=api_key,
        base_url=base_url.rstrip("/"),
        http_client=requests.Session(),
        timeout=60.0,
        max_retries=3,
        proxies=proxies if proxies else None,
    )

@handle_errors
def read_background_doc(file: Optional[Any]) -> Optional[str]:
    """Reads content from an uploaded file (docx, txt, md)."""
    if file is None:
        return None

    file_name = file.name.lower()
    if file_name.endswith('.docx'):
        return read_word(file)
    elif file_name.endswith(('.txt', '.md')):
        # For txt, md, read as plain text
        return StringIO(file.getvalue().decode("utf-8")).read()
    else:
        st.warning(f"ä¸æ”¯æŒçš„æ–‡ä»¶ç±»å‹: {file_name}ï¼Œè¯·ä¸Šä¼  .docx, .txt, æˆ– .md æ–‡ä»¶ã€‚")
        return None
def get_output_format_template() -> str:
    """è¿”å›æ ‡å‡†çš„ CSV è¾“å‡ºæ ¼å¼æ¨¡æ¿"""
    headers = ["æµ‹è¯•åç§°", "éœ€æ±‚ç¼–å·", "éœ€æ±‚æè¿°", "æµ‹è¯•æè¿°", "å‰ç½®æ¡ä»¶", "æµ‹è¯•æ­¥éª¤", "é¢„æœŸç»“æœ", "éœ€æ±‚è¿½æº¯"]
    header_line = ",".join([f'"{h}"' for h in headers])
    example_line = ",".join([f'"{h}ç¤ºä¾‹"' for h in headers])
    return f"{header_line}\n{example_line}"

def get_standard_prompt_template() -> str:
    """è¿”å›æ ‡å‡†çš„æç¤ºè¯æ¨¡æ¿"""
    return (
        "[ç³»ç»Ÿè§’è‰²]\n"
        "ä½ æ˜¯èµ„æ·±çš„OBC/CCUæµ‹è¯•å¼€å‘ä¸“å®¶ï¼Œç²¾é€šç”µåŠ›ç”µå­ã€è½¦è½½å……ç”µã€CAN/CAN-FDåè®®ã€ç¡¬ä»¶äº¤äº’ã€è¯Šæ–­ä¸å®‰å…¨ã€‚\n\n"
        "[å¯é€‰èƒŒæ™¯çŸ¥è¯†]\n"
        "å¦‚æœ‰èƒŒæ™¯çŸ¥è¯†ï¼Œè¯·å……åˆ†ç»“åˆç†è§£ã€‚\n---\n{èƒŒæ™¯çŸ¥è¯†}\n---\n\n"
        "[ä»»åŠ¡]\n"
        "é’ˆå¯¹'éœ€æ±‚æè¿°'ï¼Œç”Ÿæˆ {æ­£å‘æ•°} æ¡æ­£å‘ã€{å¼‚å¸¸æ•°} æ¡å¼‚å¸¸ã€{è¾¹ç•Œæ•°} æ¡è¾¹ç•Œæµ‹è¯•ç”¨ä¾‹ï¼ˆå…± {æ€»ç”¨ä¾‹æ•°} æ¡ï¼‰ï¼Œè¦æ±‚å¦‚ä¸‹ï¼š\n\n"
        "[CSV åˆ—é¡ºåº]\n{åˆ—åé€—å·åˆ†éš”}\n\n"
        "[ç”Ÿæˆè§„åˆ™]\n"
        "1. ä»…è¾“å‡ºåŸå§‹ CSV å†…å®¹ï¼Œä¸è¾“å‡ºä»»ä½•è§£é‡Šã€ä»£ç å—æˆ–å¤šä½™æ–‡æœ¬ã€‚\n"
        "2. æµ‹è¯•æ­¥éª¤åº”ç»†è‡´ã€å¯å¤ç°ï¼Œå•å…ƒæ ¼å†…ç”¨å…¨è§’åˆ†å·ï¼ˆï¼›ï¼‰åˆ†éš”ã€‚\n"
        "3. å‰ç½®æ¡ä»¶ä¸ºç©ºå¡«'æ— 'ï¼Œå¦‚éœ€ç‰¹å®šç¡¬ä»¶/çº¿æŸ/ç¯å¢ƒè¯·æ˜ç¡®ã€‚\n"
        "4. è¾“å…¥/å‚æ•°éœ€å…·ä½“å¯æ‰§è¡Œï¼ˆå¦‚VIN=1234, CAN_ID=0x18FF50E5, ç”µå‹=400V, ç”µæµ=50Aï¼‰ï¼Œæ¶‰åŠä¿¡å·/æŠ¥æ–‡/ç‰©ç†æ“ä½œè¦å†™æ˜ã€‚\n"
        "5. é¢„æœŸç»“æœåº”åŒ…å«å¯è§‚æµ‹é˜ˆå€¼ã€æ—¶åºã€è¯Šæ–­ç ã€åŠŸç‡/å®‰å…¨/äº’é”ç­‰åˆ¤æ®ï¼ˆå¦‚ï¼šç”µæµç¨³å®šåœ¨50AÂ±5%æŒç»­10sï¼Œæˆ–ä¸‹å‘BMSæ•…éšœç 0x1234ï¼‰ã€‚\n"
        "6. 'éœ€æ±‚ç¼–å·'åˆ—å¡«{éœ€æ±‚ç¼–å·}ï¼ˆæˆ–è‡ªåŠ¨ç”ŸæˆREQ-001/002â€¦ï¼‰ã€‚\n"
        "7. 'éœ€æ±‚æè¿°'åˆ—â‰¤50å­—ï¼Œç²¾å‡†æ¦‚æ‹¬éœ€æ±‚å…³é”®ç‚¹ã€‚\n"
        "8. 'éœ€æ±‚è¿½æº¯'åˆ—å†™æ˜è¯¥ç”¨ä¾‹éªŒè¯çš„å…·ä½“éœ€æ±‚ç‚¹ã€åè®®æ¡æ¬¾æˆ–åœºæ™¯ã€‚\n"
        "9. ç”¨ä¾‹åº”è¦†ç›–å…¸å‹æµç¨‹ã€å¼‚å¸¸åœºæ™¯ï¼ˆå¦‚é€šä¿¡ä¸¢å¸§/è¶…æ—¶/éæ³•æŠ¥æ–‡/ç¡¬ä»¶æ–­å¼€ï¼‰ã€è¾¹ç•Œæ¡ä»¶ï¼ˆå¦‚æé™ç”µå‹/æ¸©åº¦/åŠŸç‡/æ—¶åºï¼‰ã€‚\n"
        "10. OBC/CCUå…³æ³¨ï¼š\n"
        "    - å……ç”µæµç¨‹ï¼ˆæ’æªã€æˆæƒã€å¯åŠ¨ã€å®Œæˆã€æ‹”æªã€å¼‚å¸¸ä¸­æ–­ï¼‰\n"
        "    - CAN/CAN-FDæŠ¥æ–‡äº¤äº’ã€ä¿¡å·é‡‡é›†ã€è¯Šæ–­å¸§\n"
        "    - åŠŸç‡/æ¸©åº¦/ç”µæµ/ç”µå‹è¾¹ç•Œã€SOCé˜ˆå€¼\n"
        "    - æ•…éšœæ³¨å…¥ï¼ˆä¸¢å¸§ã€å»¶è¿Ÿã€çŸ­è·¯ã€ä¿¡å·å¼‚å¸¸ï¼‰\n"
        "    - å®‰å…¨äº’é”ã€ç¡¬ä»¶çŠ¶æ€æ£€æµ‹ã€è¯Šæ–­ç ä¸ŠæŠ¥\n"
        "    - æ—¶åºè¦æ±‚ï¼ˆå¦‚xx mså†…å“åº”/åŠ¨ä½œï¼‰\n"
        "    - ç‰©ç†æ“ä½œä¸äººæœºäº¤äº’ï¼ˆå¦‚æ’æ‹”æªã€æ€¥åœã€æˆæƒæµç¨‹ï¼‰\n\n"
        "[éœ€æ±‚è¾“å…¥]\n"
        "éœ€æ±‚ID: {éœ€æ±‚ç¼–å·}\n"
        "éœ€æ±‚æè¿°:\n{éœ€æ±‚å…¨æ–‡}\n\n"
        "[è¾“å‡º]\nä»…è¾“å‡º CSVï¼Œæ— å…¶ä»–æ–‡å­—ã€‚"
    )

def process_single_requirement(
    req_text: str = "",
    req_id: str = "", 
    base_url: str = "",
    model: str = "", 
    temperature: float = 0.2,
    headers: List[str] = None,
    pos_n: int = 2,
    neg_n: int = 2,
    edge_n: int = 2,
    auto_mode: bool = False,
    dyn_params: Dict[str, Any] = None,
    api_key: Optional[str] = None,
    local_model_url: Optional[str] = None,
    http_proxy: Optional[str] = None,
    https_proxy: Optional[str] = None,
    background_knowledge: Optional[str] = None
) -> None:
    """å¤„ç†å•æ¡éœ€æ±‚ç”Ÿæˆæµ‹è¯•ç”¨ä¾‹"""
    if not req_text.strip():
        st.warning("è¯·è¾“å…¥éœ€æ±‚æè¿°")
        return
        
    try:
        if auto_mode:
            local_pos, local_neg, local_edge = compute_dynamic_case_counts(
                req_text,
                dyn_params.get("min_total", 3),
                dyn_params.get("max_total", 9),
                dyn_params.get("pos_w", 3.0),
                dyn_params.get("neg_w", 2.0),
                dyn_params.get("edge_w", 2.0),
            )
            st.info(f"åŠ¨æ€åˆ†é… -> æ­£å‘:{local_pos} å¼‚å¸¸:{local_neg} è¾¹ç•Œ:{local_edge}")
        else:
            local_pos, local_neg, local_edge = pos_n, neg_n, edge_n

        prompt = build_prompt(
            req_text, 
            headers, 
            local_pos,
            local_neg, 
            local_edge,
            req_id,
            background_knowledge
        )

        text = call_model(
            model=model, 
            prompt=prompt, 
            api_key=api_key, 
            base_url=base_url, 
            temperature=temperature,
            local_model_url=local_model_url,
            http_proxy=http_proxy,
            https_proxy=https_proxy
        )
        
        if text:
            df = parse_csv_to_df(text, headers)
            if df is None or df.empty:
                st.error("è§£æå¤±è´¥")
            else:
                st.dataframe(df, use_container_width=True)
                make_excel_download(df)
                make_csv_download(df)

    except Exception as e:
        st.error(f"ç”Ÿæˆå¤±è´¥: {e}")
        if st.session_state.get("debug_mode"):
            st.exception(e)

if __name__ == '__main__':
    main()
