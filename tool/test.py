"""AI æµ‹è¯•ç”¨ä¾‹ç”Ÿæˆå™¨ (æ•´æ´é‡æ„ç‰ˆ)

ä¿ç•™åŠŸèƒ½:
 - å•æ¡/æ‰¹é‡éœ€æ±‚ç”¨ä¾‹ç”Ÿæˆ
 - èƒŒæ™¯çŸ¥è¯†æ–‡æ¡£ (docx/txt/md)
 - CSV è§£æä¸ä¸‹è½½ (Excel/CSV)

æ¨¡å‹ä¸è®¡è´¹è¯´æ˜:
 - MiMo-7B-RL: å…è´¹ (æ ‡æ³¨: å…è´¹)
 - Qwen-235B-A22B / deepseek-v3.1 / Qwen2.5-VL-72B-Instruct-AWQ: æ”¶è´¹ (æ ‡æ³¨: è®¡è´¹)

æ”¹åŠ¨æ‘˜è¦ (æœ¬æ¬¡é‡æ„):
 - ç§»é™¤ä»£ç†è®¾ç½®ä¸ç›¸å…³å‚æ•° (ç²¾ç®€ UI / é€»è¾‘)
 - ç²¾ç®€æ¨¡å‹è°ƒç”¨é€»è¾‘, ç»Ÿä¸€å¼‚å¸¸ä¸å›é€€å¤„ç†
 - ç§»é™¤æœªä½¿ç”¨çš„ mock ç”Ÿæˆå‡½æ•°ä¸æ— ç”¨ import
 - å¢åŠ æ¨¡å‹æ ‡ç­¾ (å…è´¹ / è®¡è´¹)
 - ä»£ç å—ç»“æ„åŒ–: å¸¸é‡åŒº / å·¥å…·å‡½æ•° / æ¨¡å‹è°ƒç”¨ / è§£æ / UI
"""

import re
import logging
from io import BytesIO, StringIO
from typing import List, Dict, Optional, Any, Tuple
import csv
import json
import time
import uuid
import requests
import pandas as pd
import streamlit as st
from docx import Document
from urllib.parse import urlparse
try:
    from openai import OpenAI
    import openai  # noqa
except Exception:
    OpenAI = None
    openai = None  # noqa
import os
import sys
import argparse

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

DEFAULT_HEADERS = ["æµ‹è¯•åç§°", "éœ€æ±‚ç¼–å·", "éœ€æ±‚æè¿°", "æµ‹è¯•æè¿°", "å‰ç½®æ¡ä»¶", "æµ‹è¯•æ­¥éª¤", "é¢„æœŸç»“æœ", "éœ€æ±‚è¿½æº¯"]
DEFAULT_BASE_URL = "http://model.mify.ai.srv"  # å†…éƒ¨æœåŠ¡ä¼˜å…ˆ
MAX_RETRY_ATTEMPTS = 3
MIN_PARAGRAPH_LENGTH = 10

API_KEY = "sk-HXFiS9bEeg95uypM96B6kJfKaxe3ze52FUeQEriGGaGIIefS"  # å›ºå®šç¡¬ç¼–ç ä½¿ç”¨

# æ¨¡å‹é›†åˆ (MiMo å…è´¹ / å…¶ä»–è®¡è´¹)
MODEL_MAP = {
    "MiMo-7B-RL": "MiMo-7B-RL",
    "Qwen-235B-A22B": "Qwen-235B-A22B",
    "deepseek-v3.1": "deepseek-v3.1",
    "Qwen2.5-VL-72B-Instruct-AWQ": "Qwen2.5-VL-72B-Instruct-AWQ",
}
ALLOWED_MODELS = list(MODEL_MAP.keys())  # é¡ºåºä¿æŒå£°æ˜æ¬¡åº

MODEL_PRICING_TAG = {
    "MiMo-7B-RL": "(å…è´¹)",
    "Qwen-235B-A22B": "(è®¡è´¹)",
    "deepseek-v3.1": "(è®¡è´¹)",
    "Qwen2.5-VL-72B-Instruct-AWQ": "(è®¡è´¹)",
}

# å†…éƒ¨ç½‘å…³å¯èƒ½éœ€è¦çš„è·¯ç”±å¤´ï¼ˆä¹‹å‰ç‰ˆæœ¬ä½¿ç”¨è¿‡ï¼‰
ROUTE_HEADER_VALUE = "xiaomi"  # é»˜è®¤ç”¨äº MiMo
MODEL_PROVIDER_HEADER = {
    "MiMo-7B-RL": "xiaomi",
    "Qwen-235B-A22B": "openai_api_compatible",
    "deepseek-v3.1": "openai_api_compatible",
    "Qwen2.5-VL-72B-Instruct-AWQ": "openai_api_compatible",
}

# é£ä¹¦APIç›¸å…³å¸¸é‡
FEISHU_BASE_API = os.environ.get("FEISHU_OPEN_BASE", "https://open.feishu.cn")
FEISHU_TOKEN_ENDPOINT = f"{FEISHU_BASE_API}/open-apis/auth/v3/tenant_access_token/internal"
FEISHU_USER_TOKEN_ENDPOINT = f"{FEISHU_BASE_API}/open-apis/authen/v1/access_token"
FEISHU_OAUTH_AUTHORIZE_URL = f"{FEISHU_BASE_API}/open-apis/authen/v1/authorize"
FEISHU_OAUTH_TOKEN_URL = f"{FEISHU_BASE_API}/open-apis/authen/v1/refresh_access_token"
FEISHU_DOC_ENDPOINT_TMPL = f"{FEISHU_BASE_API}/open-apis/docx/v1/documents/{{doc_id}}"
FEISHU_BLOCKS_ENDPOINT_TMPL = f"{FEISHU_BASE_API}/open-apis/docx/v1/documents/{{doc_id}}/blocks/{{block_id}}?page_size={{page_size}}&page_token={{page_token}}"

# é£ä¹¦æ–‡æ¡£å—ç±»å‹æŠ½å–ç­–ç•¥
FEISHU_INLINE_KEY_CANDIDATES = ["elements", "runs", "inlines", "text_run"]

def handle_errors(func):
    def wrapper(*args, **kwargs):
        try:
            return func(*args, **kwargs)
        except Exception as e:
            logger.exception(e)
            msg = str(e)
            low = msg.lower()
            if ('401' in msg) or ('authentication' in low) or ('invalid' in low and 'key' in low):
                st.error("è®¤è¯å¤±è´¥ï¼šè¯·ç¡®è®¤åç«¯å·²ä¸ºå½“å‰ç¡¬ç¼–ç å¯†é’¥æˆæƒã€‚")
            else:
                st.error(f"æ“ä½œå¤±è´¥: {msg}")
            return None
    return wrapper

# ===== é£ä¹¦APIè¾…åŠ©å‡½æ•° =====
def get_feishu_user_access_token(app_id: str, app_secret: str, code: str, debug: bool = False) -> str:
    """é€šè¿‡æˆæƒç è·å–é£ä¹¦ç”¨æˆ·è®¿é—®ä»¤ç‰Œ"""
    payload = {
        "grant_type": "authorization_code",
        "client_id": app_id,
        "client_secret": app_secret,
        "code": code
    }
    if debug:
        print(f"[DBG] Requesting user token with code: {code[:10]}...")
    
    try:
        resp = requests.post(FEISHU_OAUTH_TOKEN_URL, json=payload, timeout=10)
    except requests.RequestException as e:
        raise RuntimeError(f"User token request network error: {e}")
    
    if debug:
        print(f"[DBG] User token HTTP status: {resp.status_code}")
    
    if resp.status_code != 200:
        raise RuntimeError(f"User token HTTP {resp.status_code}: {resp.text[:300]}")
    
    try:
        data = resp.json()
    except ValueError:
        raise RuntimeError(f"User token response not JSON: {resp.text[:200]}")
    
    if debug:
        print(f"[DBG] User token raw JSON: {json.dumps(data, ensure_ascii=False)[:400]}")
    
    if data.get("code") != 0:
        raise RuntimeError(f"User token error code={data.get('code')} msg={data.get('msg')}")
    
    return data["data"]["access_token"]
def get_feishu_tenant_access_token(app_id: str, app_secret: str, debug: bool = False, retries: int = 3, base_delay: float = 0.8) -> str:
    payload = {"app_id": app_id, "app_secret": app_secret}
    last_err: Optional[Exception] = None
    for attempt in range(1, retries + 1):
        if debug:
            print(f"[DBG] Requesting token attempt {attempt}/{retries} -> {FEISHU_TOKEN_ENDPOINT}")
        try:
            resp = requests.post(FEISHU_TOKEN_ENDPOINT, json=payload, timeout=10)
        except requests.RequestException as e:
            last_err = RuntimeError(f"Token request network error: {e}")
            if debug:
                print(f"[DBG] Network error: {e}")
        else:
            if debug:
                print(f"[DBG] Token HTTP status: {resp.status_code}")
            if resp.status_code == 500:
                snippet = resp.text[:300]
                print(f"[WARN] Server 500. Body snippet: {snippet}")
                last_err = RuntimeError(f"Server 500 internal error (log_id maybe in snippet)")
            elif resp.status_code != 200:
                last_err = RuntimeError(f"Token HTTP {resp.status_code}: {resp.text[:300]}")
            else:
                try:
                    data = resp.json()
                except ValueError:
                    last_err = RuntimeError(f"Token response not JSON: {resp.text[:200]}")
                else:
                    if debug:
                        print(f"[DBG] Token raw JSON: {json.dumps(data, ensure_ascii=False)[:400]}")
                    code = data.get("code")
                    if code == 0:
                        return data["tenant_access_token"]
                    else:
                        last_err = RuntimeError(f"Token error code={code} msg={data.get('msg')}")
        # backoff
        if attempt < retries:
            delay = base_delay * (2 ** (attempt - 1))
            if debug:
                print(f"[DBG] Retry in {delay:.2f}s ...")
            time.sleep(delay)
    raise last_err or RuntimeError("Token acquisition failed (unknown error)")

def feishu_api_get(url: str, token: str, debug: bool = False) -> Dict:
    """é£ä¹¦API GETè¯·æ±‚"""
    headers = {"Authorization": f"Bearer {token}"}
    if debug:
        print(f"[DBG] GET {url}")
    try:
        resp = requests.get(url, headers=headers, timeout=10)
    except requests.RequestException as e:
        raise RuntimeError(f"GET {url} network error: {e}")
    if debug:
        print(f"[DBG] Response status: {resp.status_code}")
    if resp.status_code != 200:
        raise RuntimeError(f"GET {url} -> {resp.status_code}: {resp.text[:300]}")
    try:
        data = resp.json()
    except ValueError:
        raise RuntimeError(f"Response not JSON for {url}: {resp.text[:200]}")
    if debug:
        snippet = json.dumps(data, ensure_ascii=False)[:400]
        print(f"[DBG] JSON snippet: {snippet}")
    if data.get("code") not in (0, None):
        raise RuntimeError(f"API logical error code={data.get('code')} msg={data.get('msg')}")
    return data

def fetch_feishu_blocks_recursive(doc_id: str, block_id: str, token: str, depth: int = 0, max_depth: int = 8, debug: bool = False) -> List[Dict]:
    """é€’å½’è·å–é£ä¹¦æ–‡æ¡£å—å†…å®¹"""
    results: List[Dict] = []
    page_token = ""
    while True:
        url = FEISHU_BLOCKS_ENDPOINT_TMPL.format(doc_id=doc_id, block_id=block_id, page_size=200, page_token=page_token)
        data = feishu_api_get(url, token, debug=debug)
        
        # å¤„ç†APIå“åº”ç»“æ„
        if block_id == doc_id:
            # æ ¹å—ï¼šè¿”å›çš„æ˜¯å•ä¸ªblockå¯¹è±¡
            block_data = data.get("data", {}).get("block")
            if block_data:
                results.append(block_data)
                # å¤„ç†æ ¹å—çš„å­å—
                children = block_data.get("children", [])
                for child_id in children:
                    if child_id:
                        try:
                            child_blocks = fetch_feishu_blocks_recursive(doc_id, child_id, token, depth + 1, max_depth, debug=debug)
                            results.extend(child_blocks)
                        except Exception as e:
                            print(f"[WARN] fetch child {child_id} failed: {e}")
            break  # æ ¹å—æ²¡æœ‰åˆ†é¡µ
        else:
            # å­å—ï¼šä¹Ÿè¿”å›å•ä¸ªblockå¯¹è±¡
            block_data = data.get("data", {}).get("block")
            if block_data:
                results.append(block_data)
                # å¤„ç†å­å—çš„å­å—
                children = block_data.get("children", [])
                for child_id in children:
                    if child_id:
                        try:
                            child_blocks = fetch_feishu_blocks_recursive(doc_id, child_id, token, depth + 1, max_depth, debug=debug)
                            results.extend(child_blocks)
                        except Exception as e:
                            print(f"[WARN] fetch child {child_id} failed: {e}")
            break  # å­å—ä¹Ÿæ²¡æœ‰åˆ†é¡µï¼ˆè‡³å°‘åœ¨è¿™ä¸ªAPIä¸­ï¼‰
    
    return results

def extract_text_from_feishu_block(block: Dict) -> str:
    """ä»é£ä¹¦æ–‡æ¡£å—ä¸­æå–æ–‡æœ¬"""
    text_parts: List[str] = []
    
    # å¤„ç†ä¸åŒç±»å‹çš„å—
    block_type = block.get("block_type")
    
    # é¡µé¢å—ï¼ˆæ ¹å—ï¼‰
    if block_type == 1:
        page_data = block.get("page", {})
        elements = page_data.get("elements", [])
        for elem in elements:
            if isinstance(elem, dict):
                text_run = elem.get("text_run", {})
                content = text_run.get("content", "")
                if content:
                    text_parts.append(content.replace("\n", " ").strip())
    
    # æ–‡æœ¬å—
    elif block_type == 2:
        text_data = block.get("text", {})
        elements = text_data.get("elements", [])
        for elem in elements:
            if isinstance(elem, dict):
                text_run = elem.get("text_run", {})
                content = text_run.get("content", "")
                if content:
                    text_parts.append(content.replace("\n", " ").strip())
    
    # å…¶ä»–å—ç±»å‹ä¿æŒåŸæœ‰é€»è¾‘ä½œä¸ºåå¤‡
    else:
        block_content = block.get("block") or {}
        def iter_dict(d: Dict):
            for k, v in d.items():
                yield k, v
                if isinstance(v, dict):
                    for k2, v2 in iter_dict(v):
                        yield k2, v2
                elif isinstance(v, list):
                    for elem in v:
                        if isinstance(elem, dict):
                            for k3, v3 in iter_dict(elem):
                                yield k3, v3

        for k, v in iter_dict(block_content):
            if k == "text_run" and isinstance(v, dict):
                c = v.get("content")
                if c:
                    text_parts.append(c.replace("\n", " ").strip())
    
    text = " ".join([t for t in text_parts if t])
    return text.strip()

def feishu_blocks_to_markdown(blocks: List[Dict]) -> str:
    """å°†é£ä¹¦æ–‡æ¡£å—è½¬æ¢ä¸ºmarkdown"""
    lines: List[str] = []
    for b in blocks:
        t = extract_text_from_feishu_block(b)
        if not t:
            continue
        bt = str(b.get("block_type", "")).lower()
        if bt.startswith("heading") or bt == "3":  # æ ‡é¢˜å—
            level = bt[-1] if bt[-1].isdigit() else "2"
            lines.append(f"{'#'*int(level)} {t}")
        elif bt in ["bullet", "ordered", "list", "4", "5", "6"]:  # åˆ—è¡¨å—
            lines.append(f"- {t}")
        else:
            lines.append(t)
    # å»é‡è¿ç»­ç©ºè¡Œ
    cleaned: List[str] = []
    prev_blank = False
    for l in lines:
        blank = (not l.strip())
        if blank and prev_blank:
            continue
        cleaned.append(l)
        prev_blank = blank
    return "\n".join(cleaned)

def fetch_feishu_document(url_or_id: str, app_id: Optional[str] = None, app_secret: Optional[str] = None, debug: bool = False) -> str:
    """è·å–é£ä¹¦æ–‡æ¡£å†…å®¹å¹¶è½¬æ¢ä¸ºmarkdown
    
    Args:
        url_or_id: æ–‡æ¡£URLæˆ–ID
        app_id: é£ä¹¦åº”ç”¨IDï¼Œå¦‚æœä¸ºNoneåˆ™ä»ç¯å¢ƒå˜é‡è¯»å–
        app_secret: é£ä¹¦åº”ç”¨å¯†é’¥ï¼Œå¦‚æœä¸ºNoneåˆ™ä»ç¯å¢ƒå˜é‡è¯»å–
        debug: æ˜¯å¦å¯ç”¨è°ƒè¯•æ¨¡å¼
    
    Returns:
        æ–‡æ¡£å†…å®¹çš„markdownå­—ç¬¦ä¸²
    """
    try:
        # è·å–å‡­è¯ (ç¡¬ç¼–ç )
        if app_id is None:
            app_id = "cli_a85ffa34d3fad00c"
        if app_secret is None:
            app_secret = "MxD6ukGa9ZMJeGl5KicVSgNQLhnE1tcN"
        
        if not app_id or not app_secret:
            return f"ã€é£ä¹¦APIé”™è¯¯ã€‘ç¼ºå°‘FEISHU_APP_IDæˆ–FEISHU_APP_SECRETç¯å¢ƒå˜é‡"
        
        # æå–æ–‡æ¡£ID
        doc_input = url_or_id.strip()
        m = re.search(r"/(?:docx|wiki|docs)/([A-Za-z0-9]+)", doc_input)
        if m:
            doc_id = m.group(1)
        else:
            doc_id = doc_input
        
        # è·å–token
        token = get_feishu_tenant_access_token(app_id, app_secret, debug=debug)
        
        # æ£€æŸ¥æ˜¯å¦æ˜¯wikiæ–‡æ¡£
        is_wiki = "/wiki/" in doc_input
        
        if is_wiki:
            # å¯¹äºwikiæ–‡æ¡£ï¼Œç›´æ¥ä½¿ç”¨æå–çš„tokenä½œä¸ºæ–‡æ¡£token
            # ä¸å†éœ€è¦é¢å¤–çš„APIè°ƒç”¨æ¥è·å–èŠ‚ç‚¹ä¿¡æ¯
            if debug:
                print(f"[DEBUG] Wikiæ–‡æ¡£æ£€æµ‹åˆ°ï¼Œä½¿ç”¨tokenä½œä¸ºæ–‡æ¡£ID: {doc_id}")
            # doc_idå·²ç»æ˜¯æå–çš„wiki tokenï¼Œç›´æ¥ä½¿ç”¨
        
        # è·å–æ–‡æ¡£å—
        blocks = fetch_feishu_blocks_recursive(doc_id, doc_id, token, depth=0, max_depth=6, debug=debug)
        
        if debug:
            print(f"[DEBUG] è·å–åˆ° {len(blocks)} ä¸ªæ–‡æ¡£å—")
            if blocks:
                print(f"[DEBUG] ç¬¬ä¸€ä¸ªå—: {json.dumps(blocks[0], ensure_ascii=False, indent=2)}")
        
        # è½¬æ¢ä¸ºmarkdown
        md = feishu_blocks_to_markdown(blocks)
        
        if debug:
            print(f"[DEBUG] è½¬æ¢åçš„markdowné•¿åº¦: {len(md)}")
            print(f"[DEBUG] markdowné¢„è§ˆ: {md[:200]}...")
        
        return md
    
    except Exception as e:
        return f"ã€é£ä¹¦APIé”™è¯¯ã€‘{str(e)}"

def _is_valid_url(u: str) -> bool:
    try:
        p = urlparse(u.strip())
        return p.scheme in ("http", "https") and bool(p.netloc)
    except Exception:
        return False

def fetch_url_content(url: str, timeout: int = 10, max_chars: int = 12000) -> str:
    """Fetch webpage text content (very lightweight heuristic)."""
    try:
        # ç‰¹æ®Šå¤„ç†é£ä¹¦æ–‡æ¡£é“¾æ¥
        if 'feishu.cn' in url or 'larksuite' in url:
            # æ£€æŸ¥æ˜¯å¦æ˜¯æ–‡æ¡£é“¾æ¥ (æ”¯æŒdocxå’Œwiki)
            if re.search(r"/(?:docx|wiki|docs)/[A-Za-z0-9]+", url):
                try:
                    # å°è¯•ä½¿ç”¨é£ä¹¦APIè·å–å†…å®¹
                    content = fetch_feishu_document(url, debug=st.session_state.get("debug_mode", False))
                    if content and not content.startswith("ã€é£ä¹¦APIé”™è¯¯ã€‘"):
                        if len(content) > max_chars:
                            content = content[:max_chars] + "...ã€æˆªæ–­ã€‘"
                        return content
                    # å¦‚æœAPIå¤±è´¥ï¼Œå›é€€åˆ°ç½‘é¡µæŠ“å–
                except Exception as e:
                    if st.session_state.get("debug_mode"):
                        print(f"[DEBUG] é£ä¹¦APIå¤±è´¥ï¼Œå›é€€ç½‘é¡µæŠ“å–: {e}")
                    st.warning(f"é£ä¹¦APIè®¿é—®å¤±è´¥: {str(e)}ï¼Œå°è¯•ç½‘é¡µæŠ“å–æ–¹å¼")
            
            # å›é€€åˆ°æ™®é€šç½‘é¡µæŠ“å–
            r = requests.get(url, timeout=timeout, headers={"User-Agent": "TestCaseGenBot/1.0"})
            if r.status_code != 200:
                return f"ã€å¤±è´¥ {r.status_code}ã€‘{url}"
            text = r.text
            # ç®€å•å»æ ‡ç­¾
            text = re.sub(r"<script[\s\S]*?</script>", "", text, flags=re.IGNORECASE)
            text = re.sub(r"<style[\s\S]*?</style>", "", text, flags=re.IGNORECASE)
            text = re.sub(r"<[^>]+>", "\n", text)
            text = re.sub(r"\n{2,}", "\n", text)
            text = text.strip()
            if len(text) > max_chars:
                text = text[:max_chars] + "...ã€æˆªæ–­ã€‘"
            # é’ˆå¯¹é£ä¹¦åœ¨çº¿æ–‡æ¡£çš„ç‰¹æ®Šå¤„ç†
            if len(text) < 120:  # ä»ç„¶è¿‡çŸ­ï¼Œæç¤ºç”¨æˆ·ä½¿ç”¨å¯¼å‡º
                return ("ã€é£ä¹¦æ–‡æ¡£éœ€ç™»å½•æˆ–æœªå¼€æ”¾ï¼Œå»ºè®®ï¼š1) åœ¨é£ä¹¦ä¸­å¯¼å‡ºä¸º docx åä¸Šä¼ ï¼›" \
                        "2) æˆ–æä¾›å¼€æ”¾æ¥å£ Token åèµ° API æŠ“å–ã€‘" + url)
            return text
        
        # æ™®é€šç½‘é¡µå¤„ç†
        r = requests.get(url, timeout=timeout, headers={"User-Agent": "TestCaseGenBot/1.0"})
        if r.status_code != 200:
            return f"ã€å¤±è´¥ {r.status_code}ã€‘{url}"
        text = r.text
        # ç®€å•å»æ ‡ç­¾
        text = re.sub(r"<script[\s\S]*?</script>", "", text, flags=re.IGNORECASE)
        text = re.sub(r"<style[\s\S]*?</style>", "", text, flags=re.IGNORECASE)
        text = re.sub(r"<[^>]+>", "\n", text)
        text = re.sub(r"\n{2,}", "\n", text)
        text = text.strip()
        if len(text) > max_chars:
            text = text[:max_chars] + "...ã€æˆªæ–­ã€‘"
        # å…¶ä»–ç«™ç‚¹è¿‡æ»¤éå¸¸çŸ­å†…å®¹
        if len(text) < 50:
            return f"ã€å†…å®¹è¿‡çŸ­æˆ–æ— æ³•æå–ã€‘{url}"
        return text
    except Exception as e:
        return f"ã€å¼‚å¸¸: {e.__class__.__name__}ã€‘{url}"

@handle_errors
def read_word(file) -> str:
    doc = Document(file)
    paras = [p.text.strip() for p in doc.paragraphs if p.text and p.text.strip()]
    content = "\n".join(paras)
    if not content.strip():
        raise ValueError("Word æ–‡æ¡£ä¸ºç©º")
    return content

@handle_errors
def read_excel(uploaded_file) -> Dict[str, pd.DataFrame]:
    xl = pd.ExcelFile(uploaded_file)
    sheets = {}
    for sheet in xl.sheet_names:
        df = xl.parse(sheet)
        if df.empty:
            continue
        sheets[sheet] = df
    if not sheets:
        raise ValueError("Excel æ²¡æœ‰æœ‰æ•ˆå·¥ä½œè¡¨")
    return sheets

def build_prompt(requirement: str, headers: List[str], pos_n: int, neg_n: int, edge_n: int, req_id: str = "", background_knowledge: Optional[str] = None) -> str:
    if not requirement.strip():
        raise ValueError("éœ€æ±‚ä¸èƒ½ä¸ºç©º")
    cols_line = ",".join(headers)
    total_cases = pos_n + neg_n + edge_n
    background_section = ""
    if background_knowledge and background_knowledge.strip():
        background_section = f"""
è¯·å‚è€ƒä»¥ä¸‹èƒŒæ™¯çŸ¥è¯†æ¥ç”Ÿæˆç”¨ä¾‹ï¼š
---
{background_knowledge.strip()}
---
"""
    guidance = f"""
{background_section}
ä½ æ˜¯ä¸€åå…·å¤‡ç”µåŠ›ç”µå­ä¸è½¦è½½ç³»ç»Ÿç»éªŒçš„é«˜çº§æµ‹è¯•å·¥ç¨‹å¸ˆï¼Œç†Ÿæ‚‰ OBC/CCU/BMS/EVCCã€CAN/CAN-FDã€å……ç”µæµç¨‹ä¸åŠŸç‡çº¦æŸã€‚
è¯·åŸºäºä¸‹åˆ—éœ€æ±‚ç”Ÿæˆ {total_cases} æ¡é«˜è´¨é‡ã€å¯æ‰§è¡Œçš„æµ‹è¯•ç”¨ä¾‹ï¼ˆCSV æ ¼å¼ï¼Œç¬¬ä¸€è¡Œä¸ºè¡¨å¤´ï¼‰ï¼š
{cols_line}

åˆ†é…ï¼šæ­£å‘ {pos_n} æ¡ï¼Œå¼‚å¸¸ {neg_n} æ¡ï¼Œè¾¹ç•Œ {edge_n} æ¡ã€‚

è§„åˆ™ï¼š
- ä»…è¾“å‡º CSV å†…å®¹ï¼Œä¸è¦é™„åŠ è§£é‡Šæˆ–ä»£ç å—ã€‚
- æµ‹è¯•æ­¥éª¤ç”¨åˆ†å·ï¼ˆï¼›ï¼‰åˆ†éš”å¹¶æ”¾åœ¨åŒä¸€å•å…ƒæ ¼å†…ã€‚
- å‰ç½®æ¡ä»¶ä¸ºç©ºå¡«å†™ "æ— "ã€‚
- è¾“å…¥æ•°æ®è¦å…·ä½“ï¼ˆä¾‹å¦‚ï¼šVIN=1234, CAN_ID=0x18FF50E5, ç”µå‹=400V, ç”µæµ=50Aï¼‰ã€‚
- é¢„æœŸç»“æœåº”åŒ…å«å¯è§‚æµ‹çš„é˜ˆå€¼æˆ–æ—¶é—´æ¡ä»¶ï¼ˆä¾‹å¦‚ï¼šç”µæµç¨³å®šåœ¨ 50A Â±5% æŒç»­ 10sï¼‰ã€‚
- éœ€æ±‚ç¼–å·åˆ—å¡«å†™: {req_id if req_id else "REQ-001"}
- éœ€æ±‚æè¿°åˆ—ç®€è¦æ¦‚æ‹¬éœ€æ±‚å†…å®¹ï¼ˆä¸è¶…è¿‡50å­—ï¼‰
- éœ€æ±‚è¿½æº¯åˆ—å¡«å†™è¯¥æµ‹è¯•ç”¨ä¾‹éªŒè¯çš„å…·ä½“éœ€æ±‚ç‚¹

ç”µåŠ›ç”µå­æ³¨æ„äº‹é¡¹ï¼šæ˜ç¡®é‡‡æ ·æ—¶åºã€SOC/æ¸©åº¦/ç”µåŠ›è¾¹ç•Œã€æ•…éšœæ³¨å…¥ï¼ˆä¸¢å¸§/å»¶è¿Ÿ/çŸ­è·¯ï¼‰ã€EVCCé€šä¿¡åè®®å’Œå®‰å…¨äº’é”ã€‚
"""
    return f"{guidance}\n\néœ€æ±‚ID: {req_id}\néœ€æ±‚æè¿°:\n{requirement.strip()}\n\nè¯·å¼€å§‹ç”Ÿæˆæµ‹è¯•ç”¨ä¾‹ï¼š"

def get_standard_prompt_template() -> str:
    """è¿”å›åœ¨ç”Ÿæˆç”¨ä¾‹æ—¶ä½¿ç”¨çš„æ ‡å‡† Prompt æ¨¡æ¿ï¼ˆå ä½ç¬¦å½¢å¼å±•ç¤ºï¼‰ã€‚"""
    return (
        "[ç³»ç»Ÿè§’è‰²]\n"
        "ä½ æ˜¯èµ„æ·±çš„OBC/CCUæµ‹è¯•å¼€å‘ä¸“å®¶ï¼Œç²¾é€šç”µåŠ›ç”µå­ã€è½¦è½½å……ç”µã€CAN/CAN-FDåè®®ã€ç¡¬ä»¶äº¤äº’ã€è¯Šæ–­ä¸å®‰å…¨ã€‚\n\n"
        "[å¯é€‰èƒŒæ™¯çŸ¥è¯†]\n"
        "å¦‚æœ‰èƒŒæ™¯çŸ¥è¯†ï¼Œè¯·å……åˆ†ç»“åˆç†è§£ã€‚\n---\n{èƒŒæ™¯çŸ¥è¯†}\n---\n\n"
        "[ä»»åŠ¡]\n"
        "é’ˆå¯¹â€˜éœ€æ±‚æè¿°â€™ï¼Œç”Ÿæˆ {æ­£å‘æ•°} æ¡æ­£å‘ã€{å¼‚å¸¸æ•°} æ¡å¼‚å¸¸ã€{è¾¹ç•Œæ•°} æ¡è¾¹ç•Œæµ‹è¯•ç”¨ä¾‹ï¼ˆå…± {æ€»ç”¨ä¾‹æ•°} æ¡ï¼‰ï¼Œè¦æ±‚å¦‚ä¸‹ï¼š\n\n"
        "[CSV åˆ—é¡ºåº]\n{åˆ—åé€—å·åˆ†éš”}\n\n"
        "[ç”Ÿæˆè§„åˆ™]\n"
        "1. ä»…è¾“å‡ºåŸå§‹ CSV å†…å®¹ï¼Œä¸è¾“å‡ºä»»ä½•è§£é‡Šã€ä»£ç å—æˆ–å¤šä½™æ–‡æœ¬ã€‚\n"
        "2. æµ‹è¯•æ­¥éª¤åº”ç»†è‡´ã€å¯å¤ç°ï¼Œå•å…ƒæ ¼å†…ç”¨å…¨è§’åˆ†å·ï¼ˆï¼›ï¼‰åˆ†éš”ã€‚\n"
        "3. å‰ç½®æ¡ä»¶ä¸ºç©ºå¡«â€˜æ— â€™ï¼Œå¦‚éœ€ç‰¹å®šç¡¬ä»¶/çº¿æŸ/ç¯å¢ƒè¯·æ˜ç¡®ã€‚\n"
        "4. è¾“å…¥/å‚æ•°éœ€å…·ä½“å¯æ‰§è¡Œï¼ˆå¦‚VIN=1234, CAN_ID=0x18FF50E5, ç”µå‹=400V, ç”µæµ=50Aï¼‰ï¼Œæ¶‰åŠä¿¡å·/æŠ¥æ–‡/ç‰©ç†æ“ä½œè¦å†™æ˜ã€‚\n"
        "5. é¢„æœŸç»“æœåº”åŒ…å«å¯è§‚æµ‹é˜ˆå€¼ã€æ—¶åºã€è¯Šæ–­ç ã€åŠŸç‡/å®‰å…¨/äº’é”ç­‰åˆ¤æ®ï¼ˆå¦‚ï¼šç”µæµç¨³å®šåœ¨50AÂ±5%æŒç»­10sï¼Œæˆ–ä¸‹å‘BMSæ•…éšœç 0x1234ï¼‰ã€‚\n"
        "6. â€˜éœ€æ±‚ç¼–å·â€™åˆ—å¡«{éœ€æ±‚ç¼–å·}ï¼ˆæˆ–è‡ªåŠ¨ç”ŸæˆREQ-001/002â€¦ï¼‰ã€‚\n"
        "7. â€˜éœ€æ±‚æè¿°â€™åˆ—â‰¤50å­—ï¼Œç²¾å‡†æ¦‚æ‹¬éœ€æ±‚å…³é”®ç‚¹ã€‚\n"
        "8. â€˜éœ€æ±‚è¿½æº¯â€™åˆ—å†™æ˜è¯¥ç”¨ä¾‹éªŒè¯çš„å…·ä½“éœ€æ±‚ç‚¹ã€åè®®æ¡æ¬¾æˆ–åœºæ™¯ã€‚\n"
        "9. ç”¨ä¾‹åº”è¦†ç›–å…¸å‹æµç¨‹ã€å¼‚å¸¸åœºæ™¯ï¼ˆå¦‚é€šä¿¡ä¸¢å¸§/è¶…æ—¶/éæ³•æŠ¥æ–‡/ç¡¬ä»¶æ–­å¼€ï¼‰ã€è¾¹ç•Œæ¡ä»¶ï¼ˆå¦‚æé™ç”µå‹/æ¸©åº¦/åŠŸç‡/æ—¶åºï¼‰ã€‚\n"
        "10. OBC/CCUå…³æ³¨ï¼š\n"
        "    - å……ç”µæµç¨‹ï¼ˆæ’æªã€æˆæƒã€å¯åŠ¨ã€å®Œæˆã€æ‹”æªã€å¼‚å¸¸ä¸­æ–­ï¼‰\n"
        "    - CAN/CAN-FDæŠ¥æ–‡äº¤äº’ã€ä¿¡å·é‡‡é›†ã€è¯Šæ–­å¸§\n"
        "    - åŠŸç‡/æ¸©åº¦/ç”µæµ/ç”µå‹è¾¹ç•Œã€SOCé˜ˆå€¼\n"
        "    - æ•…éšœæ³¨å…¥ï¼ˆä¸¢å¸§ã€å»¶è¿Ÿã€çŸ­è·¯ã€ä¿¡å·å¼‚å¸¸ï¼‰\n"
        "    - å®‰å…¨äº’é”ã€ç¡¬ä»¶çŠ¶æ€æ£€æµ‹ã€è¯Šæ–­ç ä¸ŠæŠ¥\n"
        "    - æ—¶åºè¦æ±‚ï¼ˆå¦‚xx mså†…å“åº”/åŠ¨ä½œï¼‰\n"
        "    - ç‰©ç†æ“ä½œä¸äººæœºäº¤äº’ï¼ˆå¦‚æ’æ‹”æªã€æ€¥åœã€æˆæƒæµç¨‹ï¼‰\n\n"
        "[éœ€æ±‚è¾“å…¥]\n"
        "éœ€æ±‚ID: {éœ€æ±‚ç¼–å·}\n"
        "éœ€æ±‚æè¿°:\n{éœ€æ±‚å…¨æ–‡}\n\n"
        "[è¾“å‡º]\nä»…è¾“å‡º CSVï¼Œæ— å…¶ä»–æ–‡å­—ã€‚"
    )

def get_output_format_template(headers: List[str] = None) -> str:
    """è¿”å›æ ‡å‡†çš„è¾“å‡ºæ ¼å¼æ¨¡æ¿ï¼ˆCSVæ ¼å¼ï¼Œç¬¬ä¸€è¡Œä¸ºè¡¨å¤´ï¼Œç¬¬äºŒè¡Œä¸ºå ä½ç¬¦ç¤ºä¾‹ï¼‰ã€‚"""
    if headers is None:
        headers = DEFAULT_HEADERS
    header_line = ",".join(f'"{h}"' for h in headers)
    example_line = ",".join(f'"{h}ç¤ºä¾‹"' for h in headers)
    return f"{header_line}\n{example_line}"

REQ_ID_PATTERN = re.compile(r"\b(REQ-[A-Za-z0-9]+-\d{2,4})\b")

def extract_req_id(text: str) -> Optional[str]:
    """å°è¯•ä»éœ€æ±‚æ–‡æœ¬ä¸­æŠ½å–éœ€æ±‚ç¼–å· (æ ¼å¼ç¤ºä¾‹: REQ-OBC-001)ã€‚

    è‹¥æ‰¾åˆ°å¤šä¸ª, è¿”å›ç¬¬ä¸€ä¸ªã€‚è¿”å›ç»Ÿä¸€å¤§å†™ã€‚æœªæ‰¾åˆ°è¿”å› Noneã€‚
    """
    if not text:
        return None
    match = REQ_ID_PATTERN.search(text.upper())
    if match:
        return match.group(1).upper().rstrip(':')
    return None

# ===== åŠ¨æ€ç”¨ä¾‹æ•°é‡åˆ†é… =====
KEYWORD_WEIGHTS = {
    "å¼‚å¸¸": 1.0,
    "é”™è¯¯": 1.0,
    "æ•…éšœ": 1.1,
    "è¶…æ—¶": 0.9,
    "è¾¹ç•Œ": 0.8,
    "é™åˆ¶": 0.6,
    "ä¿æŠ¤": 0.7,
    "é™çº§": 0.9,
    "é‡è¯•": 0.8,
    "å®‰å…¨": 0.7,
    "åŠ å¯†": 0.6,
}

def _complexity_score(text: str) -> float:
    if not text:
        return 0.0
    t = text.strip()
    length = len(t)
    sentences = len(re.findall(r"[ã€‚.!?]", t)) or 1
    kw_score = 0.0
    for k, w in KEYWORD_WEIGHTS.items():
        cnt = t.count(k)
        if cnt:
            kw_score += cnt * w
    # å½’ä¸€åŒ–: è®¾è®¡ç»éªŒå‚æ•°
    base = (length / 300.0) + (sentences / 6.0) + (kw_score / 4.0)
    return min(base / 3.0, 1.0)  # é™åˆ¶ 0~1

def compute_dynamic_case_counts(text: str, min_total: int, max_total: int, pos_w: float, neg_w: float, edge_w: float) -> Tuple[int, int, int]:
    score = _complexity_score(text)
    total = int(round(min_total + (max_total - min_total) * score))
    total = max(min_total, min(total, max_total))
    weights = [max(pos_w, 0.01), max(neg_w, 0.01), max(edge_w, 0.01)]
    w_sum = sum(weights)
    raw_counts = [w / w_sum * total for w in weights]
    # åˆæ­¥å››èˆäº”å…¥
    counts = [max(1, int(round(c))) for c in raw_counts]
    # è°ƒæ•´ä½¿å¾—å’Œ=total
    diff = sum(counts) - total
    if diff != 0:
        # æ ¹æ®è¯¯å·®å¤§å°è°ƒæ•´, ä¼˜å…ˆè°ƒæ•´æœ€å¤§æˆ–æœ€å°çš„åˆ†ç±»
        for _ in range(abs(diff)):
            if diff > 0:
                # éœ€è¦å‡
                idx = counts.index(max(counts))
                if counts[idx] > 1:
                    counts[idx] -= 1
            else:
                # éœ€è¦åŠ 
                idx = counts.index(min(counts))
                counts[idx] += 1
    return counts[0], counts[1], counts[2]

# ===== å•æ¡éœ€æ±‚ -> å¤šåˆ†æ”¯è§£æ =====
BRANCH_BULLET_PATTERN = re.compile(r"^\s*(?:- |\* |\d+[).ã€]\s*|[ï¼ˆ(]\d+[)ï¼‰]\s*)")

def split_requirement_into_branches(text: str, max_branches: int = 15) -> List[Dict[str, str]]:
    """å°†å•æ¡åŸå§‹éœ€æ±‚æ‹†åˆ†ä¸ºå¤šä¸ªå¯æµ‹è¯•çš„ã€åˆ†æ”¯å­éœ€æ±‚ã€ã€‚

    è§£æç­–ç•¥ (å¯å‘å¼):
    1. ä¼˜å…ˆæŒ‰æ¢è¡Œä¸­çš„é¡¹ç›®ç¬¦å·/ç¼–å·æ‹†åˆ† (æ•°å­—. / ï¼ˆæ•°å­—ï¼‰ / - / * )
    2. è‹¥æœªæ£€æµ‹åˆ°æ˜æ˜¾æ¡ç›®, å°è¯•æŒ‰å¥å·/åˆ†å·åˆ‡æˆå¥å­ (é•¿åº¦>15) ä½œä¸ºå€™é€‰
    3. å¯¹è¿‡çŸ­ (<8) è¡Œè‡ªåŠ¨ä¸åç»­åˆå¹¶
    4. é™åˆ¶æœ€å¤§åˆ†æ”¯æ•°, è¶…è¿‡æ—¶æˆªæ–­å¹¶åœ¨æœ€åè¿½åŠ ä¸€æ¡ã€å…¶ä½™åˆå¹¶ã€
    è¿”å›: [{'branch_index':1,'branch_id':'B01','title':'...','content':'...'}]
    """
    if not text or len(text.strip()) < 8:
        return []
    raw_lines = [l.rstrip() for l in text.strip().splitlines() if l.strip()]
    candidates: List[str] = []
    buffer = []
    def flush_buffer():
        if buffer:
            merged = " ".join(buffer).strip()
            if merged:
                candidates.append(merged)
            buffer.clear()

    bullet_mode = any(BRANCH_BULLET_PATTERN.search(l) for l in raw_lines)
    if bullet_mode:
        for line in raw_lines:
            if BRANCH_BULLET_PATTERN.search(line):
                flush_buffer()
                # å»æ‰å‰ç¼€ç¬¦å·
                cleaned = BRANCH_BULLET_PATTERN.sub("", line, count=1).strip()
                buffer.append(cleaned)
            else:
                # ç»§ç»­ç´¯ç§¯åˆ°å½“å‰åˆ†æ”¯
                buffer.append(line.strip())
        flush_buffer()
    else:
        # å¥å­åˆ‡åˆ† (ä¸­æ–‡å¥å·/åˆ†å·/è‹±æ–‡æ ‡ç‚¹)
        sentences = re.split(r"(?<=[ã€‚ï¼›;.!?])\s+", text.strip())
        for s in sentences:
            s_clean = s.strip()
            if len(s_clean) >= 15:
                candidates.append(s_clean)
        # å¦‚æœè¿˜æ²¡æœ‰, æ•´ä½“ä½œä¸ºä¸€ä¸ª
        if not candidates:
            candidates = [text.strip()]

    # åˆå¹¶è¿‡çŸ­ç‰‡æ®µ (<8) åˆ°å‰ä¸€ä¸ª
    merged: List[str] = []
    for seg in candidates:
        if merged and len(seg) < 8:
            merged[-1] = merged[-1] + " " + seg
        else:
            merged.append(seg)

    # æˆªæ–­ä¸æº¢å‡ºå¤„ç†
    overflow = []
    if len(merged) > max_branches:
        overflow = merged[max_branches-1:]
        merged = merged[:max_branches-1]
        merged.append("å…¶ä½™åˆå¹¶: " + " | ".join(overflow[:5]) + (" ..." if len(overflow) > 5 else ""))

    branches: List[Dict[str, str]] = []
    for idx, seg in enumerate(merged, 1):
        title = seg[:40].replace('\n', ' ').strip()
        branches.append({
            "branch_index": idx,
            "branch_id": f"B{idx:02d}",
            "title": title,
            "content": seg.strip(),
        })
    return branches

def get_requirement_templates() -> Dict[str, str]:
    return {
        "OBC å……ç”µæµç¨‹": """
REQ-OBC-001: ã€åŠŸèƒ½ã€‘è½¦è½½å……ç”µæœº (OBC)ï¼šæ’æªæ¡æ‰‹->æˆæƒ->å……ç”µ->åœæ­¢
åœºæ™¯åŒ…æ‹¬ï¼šæ¥åœ°æ£€æµ‹ã€äº’é”ã€é™æµã€å……ç”µå®Œæˆæ£€æµ‹ä¸æ•…éšœå¤„ç†
éªŒè¯ç‚¹ï¼šæ¡æ‰‹æ—¶åºã€æˆæƒæµç¨‹ã€å……ç”µå‚æ•°åå•†ã€å¼‚å¸¸æ–­å¼€å¤„ç†
""",
        "CCU ä¸ BMS äº¤äº’": """
REQ-CCU-001: ã€åŠŸèƒ½ã€‘CCU è¯·æ±‚ BMS çŠ¶æ€ï¼ˆSOC/æ¸©åº¦/ç”µå‹/æ•…éšœç ï¼‰ï¼Œå¤„ç†è¶…æ—¶ä¸é‡è¯•
éªŒè¯ç‚¹ï¼šCANé€šä¿¡æ—¶åºã€æ•°æ®å®Œæ•´æ€§ã€è¶…æ—¶é‡è¯•æœºåˆ¶ã€æ•…éšœç è§£æ
""",
        "BMS SOC ä¸å……æ”¾ç”µç­–ç•¥": """
REQ-BMS-001: ã€åŠŸèƒ½ã€‘SOC ä¼°ç®—ã€æ¸©åº¦ç›¸å…³å……æ”¾ç”µé™åˆ¶ã€ä½ç”µé‡ä¿æŠ¤
éªŒè¯ç‚¹ï¼šSOCç²¾åº¦ã€æ¸©åº¦ä¿æŠ¤é˜ˆå€¼ã€åŠŸç‡é™åˆ¶ç®—æ³•ã€ä¿æŠ¤ç­–ç•¥è§¦å‘
""",
        "EVCC é€šä¿¡æ§åˆ¶": """
REQ-EVCC-001: ã€åŠŸèƒ½ã€‘EVCCä¸å……ç”µæ¡©é€šä¿¡ï¼šISO15118åè®®ã€æ•°å­—è¯ä¹¦éªŒè¯ã€å……ç”µå‚æ•°åå•†
éªŒè¯ç‚¹ï¼šåè®®æ¡æ‰‹ã€è¯ä¹¦é“¾éªŒè¯ã€å‚æ•°åå•†ã€é€šä¿¡å®‰å…¨æ€§
""",
        "å……ç”µè¿æ¥ä¸æ–­å¼€æµç¨‹": """
REQ-CHG-001: ã€åŠŸèƒ½ã€‘äººæœºä¸ç¡¬ä»¶äº¤äº’ï¼šæ’æªã€æˆæƒã€å¼€å§‹ã€å®Œæˆã€æ‹”æªä¸å¼ºåˆ¶ä¸­æ–­åœºæ™¯
éªŒè¯ç‚¹ï¼šç‰©ç†è¿æ¥æ£€æµ‹ã€ç”¨æˆ·æˆæƒã€å……ç”µå¯åœã€ç´§æ€¥æ–­å¼€
""",
    }

def get_requirement_examples() -> List[str]:
    return [
        "OBC: æ’æªå 5s å†…æœªæˆæƒåº”å–æ¶ˆè¯·æ±‚",
        "BMS: æ¸©åº¦>60Â°C æ—¶é™åˆ¶å……ç”µç”µæµè‡³ 0.2C",
        "CCU: BMS è¯·æ±‚è¶…æ—¶ 100ms åé‡è¯• 3 æ¬¡å¹¶è®°å½•æ•…éšœ",
    ]

@handle_errors
def call_model(model: str, prompt: str, base_url: str, temperature: float = 0.2) -> str:
    """è°ƒç”¨æ¨¡å‹: ä¼˜å…ˆ chat.completions, éœ€è¦æ—¶å›é€€ completions.

    å›é€€æ¡ä»¶: fallback é›†åˆæ¨¡å‹å‡ºç° 400 ä¸”è¿”å›å†…å®¹åŒ…å« prompt/field required/missing.
    """
    provider = MODEL_PROVIDER_HEADER.get(model, ROUTE_HEADER_VALUE)
    debug = st.session_state.get("debug_mode", False)
    actual_model = MODEL_MAP.get(model, model)

    def _chat_payload() -> dict:
        return {
            "model": actual_model,
            "messages": [
                {"role": "system", "content": "ä½ æ˜¯æµ‹è¯•ç”¨ä¾‹ç”ŸæˆåŠ©æ‰‹ï¼Œä¸¥æ ¼è¾“å‡º CSV"},
                {"role": "user", "content": prompt},
            ],
            "temperature": temperature,
            "max_tokens": 2000,
        }

    def _completions_payload() -> dict:
        return {
            "model": actual_model,
            "prompt": "ä½ æ˜¯æµ‹è¯•ç”¨ä¾‹ç”ŸæˆåŠ©æ‰‹ï¼Œä¸¥æ ¼è¾“å‡º CSVã€‚\n" + prompt,
            "temperature": temperature,
            "max_tokens": 2000,
        }

    def _do_request(url: str, payload: dict) -> requests.Response:
        headers = {
            "Content-Type": "application/json",
            "Authorization": f"Bearer {API_KEY}",
            "X-Model-Provider-Id": provider,
        }
        return requests.post(url, headers=headers, json=payload, timeout=60)

    chat_url = f"{base_url.rstrip('/')}/v1/chat/completions"
    comp_url = f"{base_url.rstrip('/')}/v1/completions"
    fallback_allowed = {"Qwen-235B-A22B", "deepseek-v3.1", "Qwen2.5-VL-72B-Instruct-AWQ"}

    # Chat è°ƒç”¨
    for attempt in range(MAX_RETRY_ATTEMPTS):
        try:
            resp = _do_request(chat_url, _chat_payload())
            if resp.status_code >= 500:
                if debug:
                    st.warning(f"[è°ƒè¯•-chat] {attempt+1} æ¬¡ -> {resp.status_code}: {resp.text[:200]}")
                if attempt < MAX_RETRY_ATTEMPTS - 1:
                    time.sleep(1.2 * (attempt + 1))
                    continue
            if resp.status_code == 400:
                low = resp.text.lower()
                if model in fallback_allowed and any(k in low for k in ["prompt", "field required", "missing"]):
                    if debug:
                        st.info("[è°ƒè¯•] Chat 400 ç¼ºå­—æ®µ, å›é€€ completions")
                    break
            resp.raise_for_status()
            data = resp.json()
            return data["choices"][0]["message"]["content"]
        except requests.exceptions.HTTPError as e:
            code = e.response.status_code if e.response else None
            if code in (502, 503, 504, 429) and attempt < MAX_RETRY_ATTEMPTS - 1:
                time.sleep(1.2 * (attempt + 1))
                continue
            if code == 400:
                if model not in fallback_allowed:
                    raise e
                break
            raise e
        except (requests.exceptions.RequestException, KeyError, IndexError) as e:
            if attempt == MAX_RETRY_ATTEMPTS - 1:
                raise e
            if debug:
                st.warning(f"[è°ƒè¯•-chat] å¼‚å¸¸é‡è¯• {attempt+1}: {e}")
            time.sleep(1.0 * (attempt + 1))
    else:
        if model not in fallback_allowed:
            raise Exception("chat.completions é‡è¯•è€—å°½")

    # å›é€€ completions
    if model in fallback_allowed:
        if debug:
            st.info(f"[è°ƒè¯•] å›é€€ completions è°ƒç”¨ {model}")
        for attempt in range(MAX_RETRY_ATTEMPTS):
            try:
                resp = _do_request(comp_url, _completions_payload())
                if resp.status_code >= 500:
                    if debug:
                        st.warning(f"[è°ƒè¯•-comp] {attempt+1} æ¬¡ -> {resp.status_code}: {resp.text[:200]}")
                    if attempt < MAX_RETRY_ATTEMPTS - 1:
                        time.sleep(1.2 * (attempt + 1))
                        continue
                resp.raise_for_status()
                data = resp.json()
                if "choices" in data and data["choices"]:
                    c0 = data["choices"][0]
                    if isinstance(c0, dict):
                        if "message" in c0 and "content" in c0["message"]:
                            return c0["message"]["content"]
                        if "text" in c0:
                            return c0["text"]
                return json.dumps(data, ensure_ascii=False)[:4000]
            except requests.exceptions.HTTPError as e:
                code = e.response.status_code if e.response else None
                if code in (502, 503, 504, 429) and attempt < MAX_RETRY_ATTEMPTS - 1:
                    time.sleep(1.2 * (attempt + 1))
                    continue
                raise e
            except (requests.exceptions.RequestException, KeyError, IndexError) as e:
                if attempt == MAX_RETRY_ATTEMPTS - 1:
                    raise e
                if debug:
                    st.warning(f"[è°ƒè¯•-comp] å¼‚å¸¸é‡è¯• {attempt+1}: {e}")
                time.sleep(1.0 * (attempt + 1))
        raise Exception("completions å›é€€ä¹Ÿå¤±è´¥")

    raise Exception("æ¨¡å‹è°ƒç”¨å¤±è´¥ (æœªå‘½ä¸­æˆåŠŸè·¯å¾„)")

@handle_errors
def parse_csv_to_df(csv_text: str, expected_headers: List[str]) -> pd.DataFrame:
    if not csv_text or not csv_text.strip(): raise ValueError("CSV å†…å®¹ä¸ºç©º")
    cleaned = csv_text.strip()
    cleaned = re.sub(r"^```.*?\n", "", cleaned, flags=re.MULTILINE)
    cleaned = re.sub(r"\n```$", "", cleaned)
    cleaned = cleaned.replace("\ufeff", "")
    lines = [l for l in cleaned.splitlines() if l.strip()]
    if not lines: raise ValueError("CSV å†…å®¹ä¸ºç©ºï¼ˆæ¸…ç†åï¼‰")
    text = "\n".join(lines)
    try:
        sniffer = csv.Sniffer(); dialect = sniffer.sniff(text[:4096], delimiters=",;\t|")
        delimiter = dialect.delimiter
    except Exception:
        delimiter = ','
    reader = csv.reader(StringIO(text), delimiter=delimiter, quotechar='"')
    rows = [r for r in reader if any(cell.strip() for cell in r)]
    if not rows: raise ValueError("CSV å†…å®¹æ— æ³•è§£æä¸ºè¡Œ")
    def _normalize_rows(rows_list, n_cols, delim):
        normalized = []
        for r in rows_list:
            r = [c.strip().strip('"') for c in r]
            if len(r) <= n_cols: normalized.append(r + [""] * (n_cols - len(r)))
            else:
                merged_last = delim.join(r[n_cols - 1:]); normalized.append(r[:n_cols - 1] + [merged_last])
        return normalized
    header = [c.strip().strip('"') for c in rows[0]]
    matches = sum(1 for h in header if any(exp in h or h in exp for exp in expected_headers))
    if matches >= max(1, len(expected_headers)//2):
        data_rows = rows[1:]
        if not all(len(r)==len(header) for r in data_rows): data_rows = _normalize_rows(data_rows, len(header), delimiter)
        df = pd.DataFrame(data_rows, columns=header)
    else:
        if all(len(r)==len(expected_headers) for r in rows):
            df = pd.DataFrame(rows, columns=expected_headers)
        else:
            normalized = _normalize_rows(rows, len(expected_headers), delimiter)
            df = pd.DataFrame(normalized, columns=expected_headers)
    return df.fillna("").astype(str)

def make_excel_download(df: pd.DataFrame, filename: str = "æµ‹è¯•ç”¨ä¾‹.xlsx") -> None:
    if df is None or (hasattr(df, "empty") and df.empty): st.warning("æ²¡æœ‰æ•°æ®å¯å¯¼å‡º"); return
    buf = BytesIO();
    with pd.ExcelWriter(buf, engine='openpyxl') as w: df.to_excel(w, index=False, sheet_name='æµ‹è¯•ç”¨ä¾‹')
    buf.seek(0)
    st.download_button("ğŸ’¾ ä¸‹è½½ Excel", data=buf, file_name=filename, mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet", key=f"dxl_{uuid.uuid4().hex}")

def make_csv_download(df: pd.DataFrame, filename: str = "æµ‹è¯•ç”¨ä¾‹.csv") -> None:
    if df is None or (hasattr(df, "empty") and df.empty): st.warning("æ²¡æœ‰æ•°æ®å¯å¯¼å‡º"); return
    csv_bytes = df.to_csv(index=False).encode("utf-8-sig")
    st.download_button("ğŸ’¾ ä¸‹è½½ CSV", data=csv_bytes, file_name=filename, mime="text/csv", key=f"dcsv_{uuid.uuid4().hex}")

def process_batch_requirements(base_url: str, requirements: List[str], headers: List[str], model: str, pos_n: int, neg_n: int, edge_n: int, temperature: float, background_knowledge: Optional[str] = None, *, dynamic: bool = False, dyn_params: Optional[Dict[str, Any]] = None) -> pd.DataFrame:
    all_cases = []
    pb = st.progress(0)
    status = st.empty()
    total = len(requirements)
    used_ids = set()
    for i, req in enumerate(requirements):
        pb.progress((i + 1) / total)
        status.text(f"å¤„ç†ä¸­ {i+1}/{total}")
        extracted = extract_req_id(req)
        if extracted:
            req_id = extracted
            if req_id in used_ids:  # ç®€å•é‡å¤å¤„ç†
                suffix = 2
                new_id = f"{req_id}-DUP{suffix}"
                while new_id in used_ids:
                    suffix += 1
                    new_id = f"{req_id}-DUP{suffix}"
                req_id = new_id
        else:
            req_id = f"REQ-{i+1:03d}"
        used_ids.add(req_id)
        local_pos, local_neg, local_edge = pos_n, neg_n, edge_n
        if dynamic:
            p = dyn_params or {}
            local_pos, local_neg, local_edge = compute_dynamic_case_counts(
                req,
                p.get("min_total", 3),
                p.get("max_total", 9),
                p.get("pos_w", 3.0),
                p.get("neg_w", 2.0),
                p.get("edge_w", 2.0),
            )
            if st.session_state.get("debug_mode"):
                st.write(f"{req_id} åŠ¨æ€åˆ†é… -> æ­£å‘:{local_pos} å¼‚å¸¸:{local_neg} è¾¹ç•Œ:{local_edge}")
        prompt = build_prompt(req, headers, local_pos, local_neg, local_edge, req_id, background_knowledge)
        text = call_model(model, prompt, base_url, temperature)
        if text:
            df = parse_csv_to_df(text, headers)
            if df is not None and not df.empty:
                if "éœ€æ±‚ç¼–å·" not in df.columns:
                    df.insert(0, "éœ€æ±‚ç¼–å·", req_id)
                else:
                    # å¡«å……ç©ºå€¼ / çº æ­£é¦–è¡Œç¼ºå¤±
                    df['éœ€æ±‚ç¼–å·'] = df['éœ€æ±‚ç¼–å·'].astype(str)
                    df['éœ€æ±‚ç¼–å·'] = df['éœ€æ±‚ç¼–å·'].where(df['éœ€æ±‚ç¼–å·'].str.strip() != "", req_id)
                if "éœ€æ±‚æè¿°" not in df.columns:
                    df.insert(1, "éœ€æ±‚æè¿°", req[:100])
                all_cases.append(df)
        if i < total - 1:
            time.sleep(2)
    pb.empty(); status.empty()
    if all_cases:
        return pd.concat(all_cases, ignore_index=True)
    raise ValueError("æœªç”Ÿæˆä»»ä½•ç”¨ä¾‹")

@handle_errors
def read_background_doc(file: Optional[Any]) -> Optional[str]:
    if file is None: return None
    name = file.name.lower()
    if name.endswith('.docx'): return read_word(file)
    if name.endswith(('.txt', '.md')): return StringIO(file.getvalue().decode("utf-8")).read()
    if name.endswith('.pdf'):
        try:
            # å°è¯•å¯¼å…¥PDFå¤„ç†åº“
            from PyPDF2 import PdfReader
            pdf = PdfReader(BytesIO(file.getvalue()))
            text = ""
            for page in pdf.pages:
                text += page.extract_text() + "\n"
            return text.strip()
        except ImportError:
            st.error("PDFå¤„ç†éœ€è¦å®‰è£… PyPDF2 åº“ã€‚è¯·è¿è¡Œ: pip install PyPDF2")
            return None
        except Exception as e:
            st.error(f"PDFè¯»å–å¤±è´¥: {e}")
            return None
    st.warning("ä¸æ”¯æŒçš„æ–‡ä»¶ç±»å‹ï¼Œè¯·ä½¿ç”¨ .docx, .txt, .md æˆ– .pdf")
    return None

def setup_sidebar() -> Tuple[str, str, float, List[str], int, int, int, bool, Dict[str, Any]]:
    with st.sidebar:
        st.header("è¿æ¥è®¾ç½®")
        st.caption("å½“å‰ä½¿ç”¨ç¡¬ç¼–ç  API Key (ç•Œé¢ä¸å†æä¾›ä¿®æ”¹)ã€‚")
        # æ¨¡å‹æ ‡ç­¾å±•ç¤º (å…è´¹ / è®¡è´¹)
        model_display = {m: f"{m} {MODEL_PRICING_TAG.get(m,'')}" for m in ALLOWED_MODELS}
        model_choice = st.selectbox("æ¨¡å‹ (MiMoå…è´¹ / å…¶ä»–è®¡è´¹)", list(model_display.keys()), format_func=lambda k: model_display[k])
        model = model_choice
        base_url = st.text_input("API Base URL", value=DEFAULT_BASE_URL)
        st.checkbox("è°ƒè¯•æ¨¡å¼", value=False, key="debug_mode", help="æ˜¾ç¤ºé‡è¯• / åŸå§‹é”™è¯¯ç‰‡æ®µï¼ŒååŠ©æ’æŸ¥ 502 ç­‰é—®é¢˜")
        temperature = st.slider("Temperature", 0.0, 1.0, 0.2, 0.05)
        st.divider(); st.header("èƒŒæ™¯çŸ¥è¯† (å¯é€‰)")
        background_doc = st.file_uploader("ä¸Šä¼ èƒŒæ™¯æ–‡æ¡£", type=["docx", "txt", "md", "pdf"])
        if background_doc:
            if st.session_state.get('last_background_doc_name') != background_doc.name:
                content = read_background_doc(background_doc)
                st.session_state['background_knowledge'] = content
                st.session_state['last_background_doc_name'] = background_doc.name
                if content:
                    st.success("å·²åŠ è½½èƒŒæ™¯")
        else:
            st.session_state.pop('background_knowledge_file', None)
            st.session_state.pop('last_background_doc_name', None)

        # ç›´æ¥æ–‡æœ¬è¾“å…¥èƒŒæ™¯çŸ¥è¯†
        st.markdown("**ç›´æ¥è¾“å…¥èƒŒæ™¯çŸ¥è¯† (ç²˜è´´æ–‡æ¡£å†…å®¹)**")
        direct_text = st.text_area("èƒŒæ™¯çŸ¥è¯†æ–‡æœ¬", placeholder="ç²˜è´´æ–‡æ¡£å†…å®¹ã€éœ€æ±‚è§„æ ¼è¯´æ˜ç­‰...", height=150, key="direct_background_text")
        if direct_text and direct_text.strip():
            st.session_state['background_knowledge'] = direct_text.strip()
            st.success("å·²è®¾ç½®èƒŒæ™¯çŸ¥è¯†æ–‡æœ¬")
        elif not background_doc and not st.session_state.get('background_urls_content'):
            st.session_state.pop('background_knowledge', None)

        # å¤šä¸ª URL è¾“å…¥
        st.markdown("**ç½‘é¡µé“¾æ¥ (æ¯è¡Œä¸€ä¸ª URLï¼Œå¯ä¸æ–‡æ¡£æ··åˆ)**")
        url_text = st.text_area("èƒŒæ™¯é“¾æ¥åˆ—è¡¨", placeholder="https://example.com/doc1\nhttps://example.com/spec", height=110)
        load_urls = st.button("åŠ è½½é“¾æ¥å†…å®¹")
        if load_urls:
            raw_urls = [u.strip() for u in url_text.splitlines() if u.strip()]
            valid_urls = [u for u in raw_urls if _is_valid_url(u)]
            bad_urls = [u for u in raw_urls if u and u not in valid_urls]
            fetched = []
            for u in valid_urls[:8]:  # é™åˆ¶æœ€å¤š 8 ä¸ªï¼Œé¿å…è¿‡æ…¢
                with st.spinner(f"æŠ“å– {u} ..."):
                    txt = fetch_url_content(u)
                fetched.append((u, txt))
            st.session_state['background_urls'] = valid_urls
            st.session_state['background_urls_content'] = fetched
            if bad_urls:
                st.warning(f"æ— æ•ˆé“¾æ¥å·²å¿½ç•¥: {len(bad_urls)}")
            st.success(f"å·²è·å– {len(fetched)} ä¸ªé“¾æ¥")

        # ç»„åˆèƒŒæ™¯ (æ–‡æ¡£ + ç›´æ¥æ–‡æœ¬ + URL)
        combined_parts = []
        if st.session_state.get('background_knowledge') and not st.session_state.get('direct_background_text'):
            # å¦‚æœæœ‰ä¸Šä¼ çš„æ–‡æ¡£å†…å®¹ä¸”æ²¡æœ‰ç›´æ¥è¾“å…¥ï¼Œåˆ™ä½¿ç”¨æ–‡æ¡£å†…å®¹
            combined_parts.append("ã€æ–‡æ¡£å†…å®¹ã€‘\n" + st.session_state['background_knowledge'])
        if st.session_state.get('direct_background_text') and st.session_state.get('direct_background_text').strip():
            combined_parts.append("ã€ç›´æ¥è¾“å…¥ã€‘\n" + st.session_state['direct_background_text'].strip())
        if st.session_state.get('background_urls_content'):
            for u, txt in st.session_state['background_urls_content']:
                combined_parts.append(f"ã€ç½‘é¡µæ‘˜å½•ã€‘{u}\n{txt}")
        combined_text = "\n\n".join(combined_parts) if combined_parts else None
        st.session_state['background_knowledge'] = combined_text

        if combined_text:
            with st.expander("æŸ¥çœ‹åˆå¹¶èƒŒæ™¯ (å‰500å­—ç¬¦)"):
                st.text(combined_text[:500] + ("..." if len(combined_text) > 500 else ""))
        st.divider(); st.header("ç”¨ä¾‹é…ç½®")
        headers_text = st.text_input("åˆ—å", value=",".join(DEFAULT_HEADERS))
        headers = [h.strip() for h in headers_text.split(",") if h.strip()]
        auto_mode = st.checkbox("æŒ‰éœ€æ±‚è‡ªåŠ¨åˆ†é…ç”¨ä¾‹æ•°é‡", value=False, help="åŸºäºéœ€æ±‚é•¿åº¦/å…³é”®è¯åŠ¨æ€ç¡®å®šæ­£å‘/å¼‚å¸¸/è¾¹ç•Œæ•°é‡")
        dyn_params: Dict[str, Any] = {}
        if auto_mode:
            c1, c2 = st.columns(2)
            with c1:
                min_total = st.number_input("æœ€å°æ€»æ•°", 3, 30, 3)
                pos_w = st.number_input("æ­£å‘æƒé‡", 0.5, 10.0, 3.0, 0.5)
            with c2:
                max_total = st.number_input("æœ€å¤§æ€»æ•°", 3, 50, 9)
                neg_w = st.number_input("å¼‚å¸¸æƒé‡", 0.5, 10.0, 2.0, 0.5)
            edge_w = st.number_input("è¾¹ç•Œæƒé‡", 0.5, 10.0, 2.0, 0.5)
            dyn_params = {"min_total": min_total, "max_total": max_total, "pos_w": pos_w, "neg_w": neg_w, "edge_w": edge_w}
            st.caption("æ ¹æ®éœ€æ±‚å¤æ‚åº¦ (é•¿åº¦/å¥å­æ•°/é£é™©å…³é”®è¯) åœ¨çº¿è®¡ç®—ç”¨ä¾‹æ•°é‡")
            # å ä½å›ºå®šå€¼ (ä¸ä¼šè¢«ä½¿ç”¨)
            pos_n = neg_n = edge_n = 0
        else:
            pos_n = st.number_input("æ­£å‘", 1, 20, 2)
            neg_n = st.number_input("å¼‚å¸¸", 1, 20, 2)
            edge_n = st.number_input("è¾¹ç•Œ", 1, 20, 2)
        st.divider()
        st.subheader("é£ä¹¦APIé…ç½® (å¯é€‰)")
        st.caption("ç”¨äºè®¿é—®é£ä¹¦æ–‡æ¡£ä½œä¸ºèƒŒæ™¯çŸ¥è¯†ã€‚éœ€è¦å…ˆåœ¨é£ä¹¦å¼€å‘è€…åå°é…ç½®åº”ç”¨å¹¶è·å–å‡­è¯ã€‚")
        st.info("ğŸ’¡ **é£ä¹¦æ–‡æ¡£è®¿é—®æç¤º**: å¦‚æœé‡åˆ°æƒé™é—®é¢˜ï¼Œå¯ä»¥ï¼š1) åœ¨é£ä¹¦ä¸­å¯¼å‡ºæ–‡æ¡£ä¸ºWord/PDFåä¸Šä¼ ï¼›2) å¤åˆ¶æ–‡æ¡£å†…å®¹ç›´æ¥ç²˜è´´åˆ°ä¸Šæ–¹æ–‡æœ¬æ¡†ï¼›3) åˆ†äº«æ–‡æ¡£ä¸ºå…¬å¼€é“¾æ¥")
        feishu_app_id = st.text_input("é£ä¹¦åº”ç”¨ID", placeholder="cli_xxx", help="ä»é£ä¹¦å¼€å‘è€…åå°è·å–")
        feishu_app_secret = st.text_input("é£ä¹¦åº”ç”¨å¯†é’¥", type="password", placeholder="xxx", help="ä»é£ä¹¦å¼€å‘è€…åå°è·å–")
        if feishu_app_id and feishu_app_secret:
            # å­˜å‚¨åˆ°ç¯å¢ƒå˜é‡æˆ–session
            os.environ["FEISHU_APP_ID"] = feishu_app_id
            os.environ["FEISHU_APP_SECRET"] = feishu_app_secret
            st.success("é£ä¹¦APIå‡­è¯å·²é…ç½®")
        elif feishu_app_id or feishu_app_secret:
            st.warning("è¯·åŒæ—¶æä¾›é£ä¹¦åº”ç”¨IDå’Œåº”ç”¨å¯†é’¥")
        else:
            st.info("æœªé…ç½®é£ä¹¦APIå‡­è¯ï¼Œå°†ä½¿ç”¨ç½‘é¡µæŠ“å–æ–¹å¼è®¿é—®é£ä¹¦æ–‡æ¡£")
        return base_url, model, temperature, headers, pos_n, neg_n, edge_n, auto_mode, dyn_params

def main():
    st.set_page_config(page_title="AI æµ‹è¯•ç”¨ä¾‹ç”Ÿæˆå™¨ (å®Œæ•´)", layout="wide")
    st.title("AI æµ‹è¯•ç”¨ä¾‹ç”Ÿæˆå™¨ - ç”µåŠ›ç”µå­")
    base_url, model, temperature, headers, pos_n, neg_n, edge_n, auto_mode, dyn_params = setup_sidebar()
    tab1, tab2, tab3 = st.tabs(["å•æ¡éœ€æ±‚", "æ‰¹é‡å¤„ç†", "å¸®åŠ©"])
    with tab1:
        st.subheader("å•æ¡éœ€æ±‚ç”Ÿæˆ")
        templates = get_requirement_templates(); opts = ["è‡ªå®šä¹‰"] + list(templates.keys())
        sel = st.selectbox("æ¨¡æ¿", opts)
        default = templates.get(sel, "") if sel != "è‡ªå®šä¹‰" else ""
        req_text = st.text_area("éœ€æ±‚æè¿°", value=default, height=220)
        req_id = st.text_input("éœ€æ±‚ç¼–å·", placeholder="ä¾‹å¦‚: REQ-OBC-001")
        st.checkbox("å¯ç”¨åˆ†æ”¯è§£æ (å¯¹å•æ¡éœ€æ±‚å†…éƒ¨å¤šç‚¹æ‹†åˆ†)", value=False, key="enable_branch_split")
        st.number_input("å•éœ€æ±‚åˆ†æ”¯æœ€å¤§æ•°", 2, 30, 10, key="branch_max")
        st.selectbox("åˆ†æ”¯ç”¨ä¾‹åˆ†é…ç­–ç•¥", ["å‡åˆ†", "å¤æ‚åº¦åŠ¨æ€", "æ‰‹åŠ¨å›ºå®š"], key="branch_strategy", help="å¯¹æ¯ä¸ªåˆ†æ”¯åˆ†é…çš„ç”¨ä¾‹æ•°é‡ç­–ç•¥")
        st.text_input("æ‰‹åŠ¨å›ºå®šåˆ†é…(æ­£,å¼‚,è¾¹) ä¾‹å¦‚: 2,1,1", key="branch_manual_counts")
        st.caption("æç¤º: è‹¥åŸéœ€æ±‚å«å¤šæ¡è§„åˆ™/æ­¥éª¤/æ¡ä»¶, å‹¾é€‰ 'å¯ç”¨åˆ†æ”¯è§£æ' è‡ªåŠ¨æ‹†æˆå­éœ€æ±‚å¹¶åˆ†åˆ«ç”Ÿæˆç”¨ä¾‹, æ”¯æŒåŠ¨æ€å¤æ‚åº¦å†åˆ†é…ã€‚")
        if st.button("ç”Ÿæˆ"):
            auto_req_id = req_id.strip() or extract_req_id(req_text) or ""
            if not req_id.strip() and auto_req_id:
                st.info(f"è‡ªåŠ¨è¯†åˆ«éœ€æ±‚ç¼–å·: {auto_req_id}")
            enable_branch = st.session_state.get("enable_branch_split", False)
            branch_strategy = st.session_state.get("branch_strategy", "å‡åˆ†")
            manual_counts_text = st.session_state.get("branch_manual_counts", "").strip()
            max_branches = st.session_state.get("branch_max", 10)

            placeholder = st.empty(); progress = st.progress(0)
            try:
                if not enable_branch:
                    local_pos, local_neg, local_edge = pos_n, neg_n, edge_n
                    if auto_mode:
                        local_pos, local_neg, local_edge = compute_dynamic_case_counts(
                            req_text,
                            dyn_params.get("min_total", 3),
                            dyn_params.get("max_total", 9),
                            dyn_params.get("pos_w", 3.0),
                            dyn_params.get("neg_w", 2.0),
                            dyn_params.get("edge_w", 2.0),
                        )
                        st.info(f"åŠ¨æ€åˆ†é… -> æ­£å‘:{local_pos} å¼‚å¸¸:{local_neg} è¾¹ç•Œ:{local_edge} (æ€»è®¡:{local_pos+local_neg+local_edge})")
                    prompt = build_prompt(req_text, headers, local_pos, local_neg, local_edge, auto_req_id, st.session_state.get('background_knowledge'))
                    placeholder.info("ç”Ÿæˆä¸­..."); progress.progress(10)
                    text = call_model(model, prompt, base_url, temperature); progress.progress(80)
                    if text:
                        df = parse_csv_to_df(text, headers); progress.progress(95)
                        if df is None or (hasattr(df, "empty") and df.empty): placeholder.error("è§£æå¤±è´¥")
                        else:
                            if "éœ€æ±‚ç¼–å·" in df.columns and auto_req_id:
                                df['éœ€æ±‚ç¼–å·'] = df['éœ€æ±‚ç¼–å·'].astype(str)
                                df['éœ€æ±‚ç¼–å·'] = df['éœ€æ±‚ç¼–å·'].where(df['éœ€æ±‚ç¼–å·'].str.strip() != "", auto_req_id)
                            elif auto_req_id and "éœ€æ±‚ç¼–å·" not in df.columns:
                                df.insert(0, "éœ€æ±‚ç¼–å·", auto_req_id)
                            st.dataframe(df, use_container_width=True)
                            make_excel_download(df)
                            make_csv_download(df)
                            progress.progress(100); placeholder.success("å®Œæˆ")
                else:
                    # åˆ†æ”¯è§£æ
                    branches = split_requirement_into_branches(req_text, max_branches=max_branches)
                    if not branches:
                        st.warning("æœªè§£æå‡ºæœ‰æ•ˆåˆ†æ”¯ï¼Œå›é€€ä¸ºæ•´ä½“ç”Ÿæˆ")
                        branches = [{"branch_index":1, "branch_id":"B01", "title":"æ•´ä½“", "content":req_text}]
                    st.info(f"è§£æå¾—åˆ° {len(branches)} ä¸ªåˆ†æ”¯")
                    # åˆ†æ”¯ç”¨ä¾‹åˆ†é…ç­–ç•¥
                    branch_cases: List[Tuple[Dict[str,str], Tuple[int,int,int]]] = []
                    # æ‰‹åŠ¨å›ºå®š
                    manual_tuple = None
                    if branch_strategy == "æ‰‹åŠ¨å›ºå®š" and manual_counts_text:
                        try:
                            parts = [int(x) for x in re.split(r"[ï¼Œ,]\s*", manual_counts_text) if x.strip()][:3]
                            if len(parts)==3 and all(p>0 for p in parts):
                                manual_tuple = tuple(parts)  # type: ignore
                        except Exception:
                            pass
                        if not manual_tuple:
                            st.warning("æ‰‹åŠ¨å›ºå®šæ ¼å¼ä¸æ­£ç¡®ï¼Œå°†å›é€€ä¸ºå‡åˆ†")
                    # é¢„è®¡ç®—å¤æ‚åº¦ç”¨äºåŠ¨æ€ç­–ç•¥
                    scores = [ _complexity_score(b['content']) for b in branches ]
                    min_total = dyn_params.get("min_total", 3)
                    max_total = dyn_params.get("max_total", 9)
                    for b, sc in zip(branches, scores):
                        if branch_strategy == "æ‰‹åŠ¨å›ºå®š" and manual_tuple:
                            branch_cases.append((b, manual_tuple))
                        elif branch_strategy == "å¤æ‚åº¦åŠ¨æ€":
                            # ä»¥åˆ†æ”¯å†…å®¹ä½œä¸ºè¾“å…¥è¿›è¡ŒåŠ¨æ€
                            lp, ln, le = compute_dynamic_case_counts(
                                b['content'],
                                min_total,
                                max_total,
                                dyn_params.get("pos_w", 3.0),
                                dyn_params.get("neg_w", 2.0),
                                dyn_params.get("edge_w", 2.0),
                            )
                            branch_cases.append((b, (lp, ln, le)))
                        else:
                            # å‡åˆ†: å¤ç”¨ä¸»é¢æ¿é…ç½®æˆ–é»˜è®¤ 2/2/1
                            if auto_mode:
                                lp, ln, le = compute_dynamic_case_counts(
                                    b['content'],
                                    min_total,
                                    max_total,
                                    dyn_params.get("pos_w", 3.0),
                                    dyn_params.get("neg_w", 2.0),
                                    dyn_params.get("edge_w", 2.0),
                                )
                            else:
                                lp, ln, le = pos_n, neg_n, edge_n
                            branch_cases.append((b, (lp, ln, le)))

                    combined_df = []
                    for idx, (b, (lp, ln, le)) in enumerate(branch_cases, 1):
                        sub_req_id = f"{auto_req_id or 'REQ-000'}-{b['branch_id']}"
                        with st.expander(f"åˆ†æ”¯ {idx}: {b['title']}  (æ­£:{lp}/å¼‚:{ln}/è¾¹:{le})"):
                            branch_prompt = build_prompt(b['content'], headers, lp, ln, le, sub_req_id, st.session_state.get('background_knowledge'))
                            st.write(b['content'])
                            try:
                                text = call_model(model, branch_prompt, base_url, temperature)
                                if text:
                                    dfb = parse_csv_to_df(text, headers)
                                    if dfb is not None and not dfb.empty:
                                        if 'éœ€æ±‚ç¼–å·' in dfb.columns:
                                            dfb['éœ€æ±‚ç¼–å·'] = dfb['éœ€æ±‚ç¼–å·'].where(dfb['éœ€æ±‚ç¼–å·'].str.strip() != "", sub_req_id)
                                        else:
                                            dfb.insert(0, 'éœ€æ±‚ç¼–å·', sub_req_id)
                                        dfb['éœ€æ±‚æè¿°'] = dfb['éœ€æ±‚æè¿°'].astype(str).where(dfb['éœ€æ±‚æè¿°'].str.strip() != "", b['title'][:50]) if 'éœ€æ±‚æè¿°' in dfb.columns else b['title'][:50]
                                        st.dataframe(dfb, use_container_width=True)
                                        combined_df.append(dfb)
                            except Exception as e:
                                st.error(f"åˆ†æ”¯ {b['branch_id']} ç”Ÿæˆå¤±è´¥: {e}")
                        progress.progress(int(idx/len(branch_cases)*100))
                        time.sleep(1)
                    if combined_df:
                        final_df = pd.concat(combined_df, ignore_index=True)
                        # ç»Ÿä¸€åˆ—åå»é‡: å¸¸è§é‡å¤/å˜ä½“åˆå¹¶
                        rename_map = {
                            'æµ‹è¯• æè¿°': 'æµ‹è¯•æè¿°', 'æµ‹è¯•è¯´æ˜': 'æµ‹è¯•æè¿°', 'æè¿°': 'æµ‹è¯•æè¿°',
                            'å‰ç½®': 'å‰ç½®æ¡ä»¶', 'å‰ææ¡ä»¶': 'å‰ç½®æ¡ä»¶', 'å‰ç½® æ¡ä»¶': 'å‰ç½®æ¡ä»¶',
                        }
                        final_df.columns = [rename_map.get(c.strip(), c.strip()) for c in final_df.columns]
                        # ç§»é™¤å…¨ç©ºåˆ—
                        empty_cols = [c for c in final_df.columns if final_df[c].astype(str).str.strip().eq('').all()]
                        if empty_cols:
                            final_df = final_df.drop(columns=empty_cols)
                        # è‹¥å‡ºç°é‡å¤åˆ—å (ä¾‹å¦‚å¤šæ¬¡è§£æå‡ºçš„â€œæµ‹è¯•æè¿°_1â€), åˆå¹¶ä¼˜å…ˆéç©º
                        deduped = {}
                        for c in final_df.columns:
                            base = c
                            if base in deduped:
                                # åˆå¹¶åˆ—
                                existing = deduped[base]
                                new_series = final_df[c].astype(str)
                                deduped[base] = existing.astype(str).where(existing.astype(str).str.strip()!='', new_series)
                            else:
                                deduped[base] = final_df[c]
                        final_df = pd.DataFrame(deduped)
                        # å¼ºåˆ¶åˆ—é¡ºåº (è‹¥å­˜åœ¨)
                        desired = ["æµ‹è¯•åç§°","éœ€æ±‚ç¼–å·","éœ€æ±‚æè¿°","æµ‹è¯•æè¿°","å‰ç½®æ¡ä»¶","æµ‹è¯•æ­¥éª¤","é¢„æœŸç»“æœ","éœ€æ±‚è¿½æº¯"]
                        ordered = [c for c in desired if c in final_df.columns]
                        tail = [c for c in final_df.columns if c not in ordered]
                        final_df = final_df[ordered + tail]
                        st.subheader("åˆå¹¶ç»“æœ")
                        st.dataframe(final_df, use_container_width=True)
                        make_excel_download(final_df, "æµ‹è¯•ç”¨ä¾‹_åˆ†æ”¯åˆå¹¶.xlsx")
                        make_csv_download(final_df, "æµ‹è¯•ç”¨ä¾‹_åˆ†æ”¯åˆå¹¶.csv")
                        placeholder.success("å…¨éƒ¨åˆ†æ”¯å®Œæˆ")
                    else:
                        placeholder.error("æœªç”Ÿæˆä»»ä½•åˆ†æ”¯ç”¨ä¾‹")
            finally:
                progress.empty(); placeholder.empty()
    with tab2:
        st.subheader("æ‰¹é‡å¯¼å…¥ (Excel / Word)")
        uploaded = st.file_uploader("ä¸Šä¼ æ–‡ä»¶", type=["xlsx", "docx"])
        collected: List[str] = []
        source_counts = []

        # 1. å¤„ç†æ–‡ä»¶æ¥æº
        if uploaded:
            if uploaded.name.lower().endswith('.xlsx'):
                sheets = read_excel(uploaded)
                if sheets:
                    sheet = st.selectbox("é€‰æ‹©å·¥ä½œè¡¨", list(sheets.keys()))
                    df_sheet = sheets[sheet]; st.dataframe(df_sheet.head(10))
                    col = st.selectbox("éœ€æ±‚åˆ—", list(df_sheet.columns))
                    rows = df_sheet[col].dropna().astype(str).str.strip()
                    excel_reqs = [r for r in rows if len(r) > MIN_PARAGRAPH_LENGTH]
                    collected.extend(excel_reqs)
                    source_counts.append(f"Excel:{len(excel_reqs)}")
            else:
                content = read_word(uploaded)
                if content:
                    parts = re.split(r"\n\s*\n+", content.strip())
                    word_reqs = [p for p in parts if len(p.strip()) > MIN_PARAGRAPH_LENGTH]
                    collected.extend(word_reqs)
                    source_counts.append(f"Word:{len(word_reqs)}")

        st.divider()
        # 2. æ‰‹å·¥æ–‡æœ¬ (ä¸€è¡Œä¸€ä¸ªéœ€æ±‚)
        st.markdown("**æ‰‹å·¥è¾“å…¥éœ€æ±‚ (æ¯è¡Œä¸€ä¸ª)**")
        manual_text = st.text_area("æ‰‹å·¥éœ€æ±‚åˆ—è¡¨", placeholder="éœ€æ±‚1...\néœ€æ±‚2...", height=150)
        if manual_text:
            manual_list = [l.strip() for l in manual_text.splitlines() if len(l.strip()) > MIN_PARAGRAPH_LENGTH]
            if manual_list:
                collected.extend(manual_list)
                source_counts.append(f"æ‰‹å·¥:{len(manual_list)}")

        st.divider()
        # 3. ç½‘é¡µé“¾æ¥ -> éœ€æ±‚æå– (ç®€å•æŒ‰æ®µè½æ‹†åˆ†)
        st.markdown("**ç½‘é¡µé“¾æ¥ (éœ€æ±‚æ¥æº) æ¯è¡Œä¸€ä¸ª URL**")
        url_require_text = st.text_area("éœ€æ±‚é“¾æ¥åˆ—è¡¨", placeholder="https://example.com/page1\nhttps://example.com/page2", height=110, key="req_url_box")
        fetch_req_urls = st.button("æŠ“å–é“¾æ¥éœ€æ±‚")
        if fetch_req_urls:
            raw_urls = [u.strip() for u in url_require_text.splitlines() if u.strip()]
            valid_urls = [u for u in raw_urls if _is_valid_url(u)]
            fetched_req = []
            for u in valid_urls[:6]:  # é™åˆ¶ 6 ä¸ªé¿å…è¶…æ—¶
                with st.spinner(f"æŠ“å– {u} ..."):
                    txt = fetch_url_content(u, max_chars=16000)
                # ç²—åˆ†æ®µ
                segments = re.split(r"\n\s*\n+", txt)
                seg_clean = [s.strip() for s in segments if len(s.strip()) > MIN_PARAGRAPH_LENGTH]
                # é™åˆ¶æ¯ä¸ªé“¾æ¥æœ€å¤§æ®µæ•° 25
                seg_clean = seg_clean[:25]
                if seg_clean:
                    fetched_req.extend(seg_clean)
            if fetched_req:
                # å­˜å…¥ sessionï¼Œå…è®¸é‡å¤ç‚¹å‡»è¦†ç›–
                st.session_state['batch_url_requirements'] = fetched_req
                st.success(f"é“¾æ¥å…±æå– {len(fetched_req)} æ¡å€™é€‰éœ€æ±‚")
            else:
                st.warning("æœªä»é“¾æ¥ä¸­æå–åˆ°æœ‰æ•ˆéœ€æ±‚")

        if st.session_state.get('batch_url_requirements'):
            url_count = len(st.session_state['batch_url_requirements'])
            source_counts.append(f"ç½‘é¡µ:{url_count}")
            with st.expander(f"æŸ¥çœ‹é“¾æ¥æå–éœ€æ±‚ ({url_count})"):
                for i, rtxt in enumerate(st.session_state['batch_url_requirements'][:50]):
                    st.write(f"{i+1}. {rtxt[:160]}{'...' if len(rtxt)>160 else ''}")
            collected.extend(st.session_state['batch_url_requirements'])

        # å»é‡ & æ¸…ç†
        unique_reqs = []
        seen = set()
        for r in collected:
            key = r.strip()
            if key not in seen:
                seen.add(key)
                unique_reqs.append(key)

        st.info(f"æ¥æºç»Ÿè®¡: {' | '.join(source_counts) if source_counts else 'æ— '} | åˆå¹¶åå»é‡: {len(unique_reqs)} æ¡")

        # æ‰¹é‡ç”ŸæˆæŒ‰é’®
        if st.button("æ‰¹é‡ç”Ÿæˆ (æ··åˆæ¥æº)"):
            if not unique_reqs:
                st.error("æ²¡æœ‰å¯ç”¨éœ€æ±‚")
            else:
                df_all = process_batch_requirements(
                    base_url,
                    unique_reqs,
                    headers,
                    model,
                    pos_n,
                    neg_n,
                    edge_n,
                    temperature,
                    st.session_state.get('background_knowledge'),
                    dynamic=auto_mode,
                    dyn_params=dyn_params,
                )
                st.dataframe(df_all)
                make_excel_download(df_all, "æµ‹è¯•ç”¨ä¾‹_æ‰¹é‡.xlsx")
                make_csv_download(df_all, "æµ‹è¯•ç”¨ä¾‹_æ‰¹é‡.csv")
    with tab3:
        st.subheader("ç¤ºä¾‹ä¸æœ€ä½³å®è·µ")
        for ex in get_requirement_examples(): st.write(f"- {ex}")
        st.markdown("---")
        st.subheader("èƒŒæ™¯çŸ¥è¯†è¾“å…¥æ–¹å¼")
        st.markdown("""
        **æ”¯æŒçš„è¾“å…¥æ–¹å¼ï¼š**
        - ğŸ“„ **ä¸Šä¼ æ–‡ä»¶**: æ”¯æŒ .docx, .txt, .md, .pdf æ ¼å¼
        - ğŸ“ **ç›´æ¥ç²˜è´´**: å¤åˆ¶æ–‡æ¡£å†…å®¹ç›´æ¥ç²˜è´´åˆ°æ–‡æœ¬æ¡†
        - ğŸŒ **ç½‘é¡µé“¾æ¥**: è¾“å…¥æ–‡æ¡£URLï¼Œè‡ªåŠ¨æŠ“å–å†…å®¹
        - ğŸª¶ **é£ä¹¦æ–‡æ¡£**: é€šè¿‡APIè®¿é—®æˆ–å¯¼å‡ºåä¸Šä¼ 
        
        **é£ä¹¦æ–‡æ¡£è®¿é—®é—®é¢˜è§£å†³ï¼š**
        - **æƒé™ä¸è¶³**: ä½¿ç”¨ tenant_access_token åªèƒ½è®¿é—®å…¬å¼€æ–‡æ¡£
        - **æ›¿ä»£æ–¹æ¡ˆ**: 
          1. åœ¨é£ä¹¦ä¸­å¯¼å‡ºä¸º Word/PDF â†’ ä¸Šä¼ æ–‡ä»¶
          2. å¤åˆ¶æ–‡æ¡£å†…å®¹ â†’ ç›´æ¥ç²˜è´´åˆ°æ–‡æœ¬æ¡†
          3. è®¾ç½®æ–‡æ¡£ä¸ºå…¬å¼€åˆ†äº« â†’ ä½¿ç”¨ç½‘é¡µé“¾æ¥è¾“å…¥
        """)
        st.markdown("---")
        st.subheader("æ ‡å‡†è¾“å‡ºæ ¼å¼æ¨¡æ¿")
        output_tpl = get_output_format_template()
        st.code(output_tpl, language="csv")
        st.caption("è¿™æ˜¯ç”Ÿæˆçš„æµ‹è¯•ç”¨ä¾‹CSVçš„æ ‡å‡†æ ¼å¼ï¼Œç¬¬ä¸€è¡Œä¸ºè¡¨å¤´ï¼Œç¬¬äºŒè¡Œä¸ºå ä½ç¬¦ç¤ºä¾‹ã€‚")
    st.markdown("---")
    st.subheader("æ ‡å‡† Prompt æ¨¡æ¿")
    tpl = get_standard_prompt_template()
    st.code(tpl, language="text")
    st.caption("å ä½ç¬¦ç¤ºä¾‹: {èƒŒæ™¯çŸ¥è¯†} / {åˆ—åé€—å·åˆ†éš”} / {éœ€æ±‚ç¼–å·} / {éœ€æ±‚å…¨æ–‡} / {æ­£å‘æ•°} / {å¼‚å¸¸æ•°} / {è¾¹ç•Œæ•°} / {æ€»ç”¨ä¾‹æ•°}")
    st.caption("æ¨¡å‹è®¡è´¹: MiMo-7B-RL å…è´¹; å…¶ä½™ (Qwen / Deepseek / Qwen2.5-VL) è®¡è´¹ | ä½¿ç”¨å›ºå®šå†…éƒ¨ API Key")

if __name__ == '__main__':
    main()